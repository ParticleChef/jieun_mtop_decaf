{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numpy.errstate at 0x7f11022f4208>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from coffea import hist, nanoevents, util\n",
    "from coffea.util import load, save\n",
    "import coffea.processor as processor\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import re\n",
    "import itertools\n",
    "# import vector as vec\n",
    "from coffea.nanoevents.methods import vector, candidate\n",
    "from coffea.nanoevents import NanoAODSchema, BaseSchema\n",
    "from coffea.lumi_tools import LumiMask\n",
    "# for applying JECs\n",
    "from coffea.lookup_tools import extractor\n",
    "from coffea.jetmet_tools import FactorizedJetCorrector, JetCorrectionUncertainty\n",
    "from coffea.jetmet_tools import JECStack, CorrectedJetsFactory\n",
    "from coffea.jetmet_tools import JetResolution, JetResolutionScaleFactor\n",
    "# from jmeCorrections import JetMetCorrections\n",
    "import json\n",
    "\n",
    "\n",
    "import coffea.processor as processor\n",
    "from coffea import hist\n",
    "from coffea.analysis_tools import PackedSelection, Weights\n",
    "from coffea.nanoevents import NanoAODSchema, NanoEventsFactory\n",
    "from coffea.nanoevents.methods import nanoaod\n",
    "\n",
    "NanoAODSchema.warn_missing_crossrefs = False\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "np.errstate(invalid='ignore', divide='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisProcessor(processor.ProcessorABC):\n",
    "\n",
    "    lumis = {  # Values from https://twiki.cern.ch/twiki/bin/viewauth/CMS/PdmVAnalysisSummaryTable\n",
    "        '2016': 19.52, #preVFP\n",
    "#         '2016': 16.81, #postVFP\n",
    "        '2017': 40.66,\n",
    "        '2018': 59.74\n",
    "    }\n",
    "\n",
    "    met_filter_flags = {\n",
    "\n",
    "        '2016': ['goodVertices',\n",
    "                 'globalSuperTightHalo2016Filter',\n",
    "                 'HBHENoiseFilter',\n",
    "                 'HBHENoiseIsoFilter',\n",
    "                 'EcalDeadCellTriggerPrimitiveFilter',\n",
    "                 'BadPFMuonFilter'\n",
    "                 ],\n",
    "\n",
    "        '2017': ['goodVertices',\n",
    "                 'globalSuperTightHalo2016Filter',\n",
    "                 'HBHENoiseFilter',\n",
    "                 'HBHENoiseIsoFilter',\n",
    "                 'EcalDeadCellTriggerPrimitiveFilter',\n",
    "                 'BadPFMuonFilter',\n",
    "                 'ecalBadCalibFilter'\n",
    "                 ],\n",
    "\n",
    "        '2018': ['goodVertices',\n",
    "                 'globalSuperTightHalo2016Filter',\n",
    "                 'HBHENoiseFilter',\n",
    "                 'HBHENoiseIsoFilter',\n",
    "                 'EcalDeadCellTriggerPrimitiveFilter',\n",
    "                 'BadPFMuonFilter',\n",
    "                 'ecalBadCalibFilter'\n",
    "                 ]\n",
    "    }\n",
    "\n",
    "    def __init__(self, year, xsec, corrections, ids, common):\n",
    "\n",
    "        self._fields = \"\"\"\n",
    "        CaloMET_pt\n",
    "        CaloMET_phi\n",
    "        Electron_charge\n",
    "        Electron_cutBased\n",
    "        Electron_dxy\n",
    "        Electron_dz\n",
    "        Electron_eta\n",
    "        Electron_mass\n",
    "        Electron_phi\n",
    "        Electron_pt\n",
    "        Flag_BadPFMuonFilter\n",
    "        Flag_EcalDeadCellTriggerPrimitiveFilter\n",
    "        Flag_HBHENoiseFilter\n",
    "        Flag_HBHENoiseIsoFilter\n",
    "        Flag_globalSuperTightHalo2016Filter\n",
    "        Flag_goodVertices\n",
    "        GenPart_eta\n",
    "        GenPart_genPartIdxMother\n",
    "        GenPart_pdgIdGenPart_phi\n",
    "        GenPart_pt\n",
    "        GenPart_statusFlags\n",
    "        HLT_Ele115_CaloIdVT_GsfTrkIdT\n",
    "        HLT_Ele32_WPTight_Gsf\n",
    "        HLT_PFMETNoMu120_PFMHTNoMu120_IDTight\n",
    "        HLT_PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60\n",
    "        HLT_Photon200\n",
    "        Jet_btagDeepB\n",
    "        Jet_btagDeepFlavB\n",
    "        Jet_chEmEF\n",
    "        Jet_chHEF\n",
    "        Jet_eta\n",
    "        Jet_hadronFlavour\n",
    "        Jet_jetId\n",
    "        Jet_mass\n",
    "        Jet_neEmEF\n",
    "        Jet_neHEF\n",
    "        Jet_phi\n",
    "        Jet_pt\n",
    "        Jet_rawFactor\n",
    "        MET_phi\n",
    "        MET_pt\n",
    "        Muon_charge\n",
    "        Muon_eta\n",
    "        Muon_looseId\n",
    "        Muon_mass\n",
    "        Muon_pfRelIso04_all\n",
    "        Muon_phi\n",
    "        Muon_pt\n",
    "        Muon_tightId\n",
    "        PV_npvs\n",
    "        Photon_eta\n",
    "        Photon_phi\n",
    "        Photon_pt\n",
    "        Tau_eta\n",
    "        Tau_idDecayMode\n",
    "        Tau_idMVAoldDM2017v2\n",
    "        Tau_phi\n",
    "        Tau_pt\n",
    "        fixedGridRhoFastjetAll\n",
    "        genWeight\n",
    "        nElectron\n",
    "        nGenPart\n",
    "        nJet\n",
    "        nMuon\n",
    "        nPhoton\n",
    "        nTau\n",
    "        \"\"\".split()\n",
    "\n",
    "        self._year = year\n",
    "\n",
    "        self._lumi = 1000.*float(AnalysisProcessor.lumis[year])\n",
    "\n",
    "        self._xsec = xsec\n",
    "        \n",
    "        self._samples = {\n",
    "            'sr':('ZJets','WJets','DY','TT','ST','WW','WZ','ZZ','QCD','SingleElectron','MET','Mphi'),\n",
    "            'wmcr':('WJets','DY','TT','ST','WW','WZ','ZZ','QCD','MET'),\n",
    "            'tmcr':('WJets','DY','TT','ST','WW','WZ','ZZ','QCD','MET'),\n",
    "            'wecr':('WJets','DY','TT','ST','WW','WZ','ZZ','QCD','SingleElectron','EGamma'),\n",
    "            'tecr':('WJets','DY','TT','ST','WW','WZ','ZZ','QCD','SingleElectron','EGamma'),\n",
    "            'zmcr':('WJets','DY','TT','ST','WW','WZ','ZZ','QCD','MET'),\n",
    "            'zecr':('WJets','DY','TT','ST','WW','WZ','ZZ','QCD','SingleElectron','EGamma'),\n",
    "            'gcr':('GJets','G1Jet','QCD','SinglePhoton','EGamma')\n",
    "        }\n",
    "\n",
    "        self._gentype_map = {\n",
    "            'xbb':      1,\n",
    "            'tbcq':     2,\n",
    "            'tbqq':     3,\n",
    "            'zcc':      4,\n",
    "            'wcq':      5,\n",
    "            'vqq':      6,\n",
    "            'bb':       7,\n",
    "            'bc':       8,\n",
    "            'b':        9,\n",
    "            'cc':     10,\n",
    "            'c':       11,\n",
    "            'other':   12\n",
    "            # 'garbage': 13\n",
    "        }\n",
    "\n",
    "        self._TvsQCDwp = {\n",
    "            '2016': 0.53,\n",
    "            '2017': 0.61,\n",
    "            '2018': 0.65\n",
    "        }\n",
    "\n",
    "        self._met_triggers = {\n",
    "            '2016': [\n",
    "                'PFMETNoMu90_PFMHTNoMu90_IDTight',\n",
    "                'PFMETNoMu100_PFMHTNoMu100_IDTight',\n",
    "                'PFMETNoMu110_PFMHTNoMu110_IDTight',\n",
    "                'PFMETNoMu120_PFMHTNoMu120_IDTight'\n",
    "            ],\n",
    "            '2017': [\n",
    "                'PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60',\n",
    "                'PFMETNoMu120_PFMHTNoMu120_IDTight'\n",
    "            ],\n",
    "            '2018': [\n",
    "                'PFMETNoMu120_PFMHTNoMu120_IDTight_PFHT60',\n",
    "                'PFMETNoMu120_PFMHTNoMu120_IDTight'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self._singlephoton_triggers = {\n",
    "            '2016': [\n",
    "                'Photon175',\n",
    "                'Photon165_HE10'\n",
    "            ],\n",
    "            '2017': [\n",
    "                'Photon200'\n",
    "            ],\n",
    "            '2018': [\n",
    "                'Photon200'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self._singleelectron_triggers = {  # 2017 and 2018 from monojet, applying dedicated trigger weights\n",
    "            '2016': [\n",
    "                'Ele27_WPTight_Gsf',\n",
    "                 'Ele105_CaloIdVT_GsfTrkIdT'\n",
    "#                'Ele115_CaloIdVT_GsfTrkIdT'\n",
    "#                 'Ele50_CaloIdVT_GsfTrkIdT_PFJet165'\n",
    "            ],\n",
    "            '2017': [\n",
    "                'Ele35_WPTight_Gsf',\n",
    "                'Ele115_CaloIdVT_GsfTrkIdT',\n",
    "                'Photon200'\n",
    "            ],\n",
    "            '2018': [\n",
    "                'Ele32_WPTight_Gsf',\n",
    "                'Ele115_CaloIdVT_GsfTrkIdT',\n",
    "                'Photon200'\n",
    "            ]\n",
    "        }\n",
    "        self._singlemuon_triggers = {\n",
    "            '2016': [\n",
    "                'IsoMu24',\n",
    "                'IsoTkMu24',\n",
    "                'Mu50',\n",
    "                'TkMu50'\n",
    "\n",
    "            ],\n",
    "            '2017':\n",
    "                [\n",
    "                'IsoMu27',\n",
    "                'Mu50',\n",
    "                'OldMu100',\n",
    "                'TkMu100'\n",
    "            ],\n",
    "            '2018':\n",
    "                [\n",
    "                'IsoMu24',\n",
    "                'Mu50',\n",
    "                'OldMu100',\n",
    "                'TkMu100'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self._jec = {\n",
    "\n",
    "            '2016': [ \n",
    "                'Summer16_07Aug2017_V11_MC_L1FastJet_AK4PFPuppi',\n",
    "                'Summer16_07Aug2017_V11_MC_L2L3Residual_AK4PFPuppi',\n",
    "                'Summer16_07Aug2017_V11_MC_L2Relative_AK4PFPuppi',\n",
    "                'Summer16_07Aug2017_V11_MC_L2Residual_AK4PFPuppi',\n",
    "                'Summer16_07Aug2017_V11_MC_L3Absolute_AK4PFPuppi'\n",
    "            ],\n",
    "\n",
    "            '2017': [\n",
    "                'Fall17_17Nov2017_V32_MC_L1FastJet_AK4PFPuppi',\n",
    "                'Fall17_17Nov2017_V32_MC_L2L3Residual_AK4PFPuppi',\n",
    "                'Fall17_17Nov2017_V32_MC_L2Relative_AK4PFPuppi',\n",
    "                'Fall17_17Nov2017_V32_MC_L2Residual_AK4PFPuppi',\n",
    "                'Fall17_17Nov2017_V32_MC_L3Absolute_AK4PFPuppi'\n",
    "            ],\n",
    "\n",
    "            '2018': [\n",
    "                'Autumn18_V19_MC_L1FastJet_AK4PFPuppi',\n",
    "                'Autumn18_V19_MC_L2L3Residual_AK4PFPuppi',\n",
    "                'Autumn18_V19_MC_L2Relative_AK4PFPuppi',  # currently broken\n",
    "                'Autumn18_V19_MC_L2Residual_AK4PFPuppi',\n",
    "                'Autumn18_V19_MC_L3Absolute_AK4PFPuppi'\n",
    "            ]\n",
    "        }\n",
    "# not updated JUNC\n",
    "        self._junc = {\n",
    "\n",
    "            '2016': \n",
    "                'Summer16_07Aug2017_V11_MC_Uncertainty_AK4PFPuppi',\n",
    "            \n",
    "\n",
    "            '2017': [\n",
    "                'Fall17_17Nov2017_V32_MC_Uncertainty_AK4PFPuppi'\n",
    "            ],\n",
    "\n",
    "            '2018': [\n",
    "                'Autumn18_V19_MC_Uncertainty_AK4PFPuppi'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self._jr = {\n",
    "\n",
    "            '2016': \n",
    "                'Summer16_25nsV1b_MC_PtResolution_AK4PFPuppi',\n",
    "\n",
    "\n",
    "            '2017': [\n",
    "                'Fall17_V3b_MC_PtResolution_AK4PFPuppi'\n",
    "            ],\n",
    "\n",
    "            '2018': [\n",
    "                'Autumn18_V7b_MC_PtResolution_AK4PFPuppi'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self._jersf = {\n",
    "\n",
    "            '2016': \n",
    "                'Summer16_25nsV1b_MC_SF_AK4PFPuppi',\n",
    "            \n",
    "\n",
    "            '2017': [\n",
    "                'Fall17_V3b_MC_SF_AK4PFPuppi'\n",
    "            ],\n",
    "\n",
    "            '2018': [\n",
    "                'Autumn18_V7b_MC_SF_AK4PFPuppi'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self._corrections = corrections\n",
    "        self._ids = ids\n",
    "        self._common = common\n",
    "\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'sumw': hist.Hist(\n",
    "                'sumw',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Bin('sumw', 'Weight value', [0.])),\n",
    "            \n",
    "            'template': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Cat('systematic', 'Systematic'),\n",
    "                hist.Bin('mT', '$m_{T}$ [GeV]', 20, 0, 600)),\n",
    "            \n",
    "            'mT': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('mT', '$m_{T}$ [GeV]', 20, 0, 600)),\n",
    "            \n",
    "            'recoil': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('recoil', 'Hadronic Recoil', [250.0, 280.0, 310.0, 340.0, 370.0, 400.0, 430.0, 470.0, 510.0, 550.0, 590.0, 640.0, 690.0, 740.0, 790.0, 840.0, 900.0, 960.0, 1020.0, 1090.0, 1160.0, 1250.0, 3000])),\n",
    "\n",
    "            'eT_miss': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('eT_miss', '$E^T_{miss}$[GeV]', 20, 0, 600)),\n",
    "\n",
    "            'ele_pT': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('ele_pT', 'Tight electron $p_{T}$ [GeV]', 10, 0, 200)),\n",
    "\n",
    "            'mu_pT': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('mu_pT', 'Tight Muon $p_{T}$ [GeV]', 10, 0, 200)),\n",
    "            \n",
    "            'j1pt': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('j1pt','AK4 Leading Jet Pt',[30.0, 60.0, 90.0, 120.0, 150.0, 180.0, 210.0, 250.0, 280.0, 310.0, 340.0, 370.0, 400.0, 430.0, 470.0, 510.0, 550.0, 590.0, 640.0, 690.0, 740.0, 790.0, 840.0, 900.0, 960.0, 1020.0, 1090.0, 1160.0, 1250.0])\n",
    "            ),\n",
    "            'j1eta': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('j1eta','AK4 Leading Jet Eta',35,-3.5,3.5)),\n",
    "            \n",
    "            'j1phi': hist.Hist(\n",
    "                'Events', \n",
    "                hist.Cat('dataset', 'Dataset'), \n",
    "                hist.Cat('region', 'Region'), \n",
    "                hist.Bin('j1phi','AK4 Leading Jet Phi',35,-3.5,3.5)),\n",
    "\n",
    "            'fj1pt': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('fj1pt','AK15 Leading Jet Pt',[30.0, 60.0, 90.0, 120.0, 150.0, 180.0, 210.0, 250.0, 280.0, 310.0, 340.0, 370.0, 400.0, 430.0, 470.0, 510.0, 550.0, 590.0, 640.0, 690.0, 740.0, 790.0, 840.0, 900.0, 960.0, 1020.0, 1090.0, 1160.0, 1250.0])\n",
    "            ),\n",
    "            'fj1eta': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('fj1eta','AK15 Leading Jet Eta',35,-3.5,3.5)),\n",
    "            \n",
    "            'fj1phi': hist.Hist(\n",
    "                'Events', \n",
    "                hist.Cat('dataset', 'Dataset'), \n",
    "                hist.Cat('region', 'Region'), \n",
    "                hist.Bin('fj1phi','AK15 Leading Jet Phi',35,-3.5,3.5)),\n",
    "\n",
    "            'dphi_e_etmiss': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('dphi_e_etmiss', '$\\Delta\\phi (e, E^T_{miss} )$', 30, 0, 3.5)),\n",
    "            \n",
    "            'dphi_mu_etmiss': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('dphi_mu_etmiss','$\\Delta\\phi (\\mu, E^T_{miss} )$', 30, 0, 3.5)),\n",
    "            \n",
    "            'ndflvL': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('ndflvL', 'AK4 Number of deepFlavor Loose Jets', 6, -0.5, 5.5)),\n",
    "            'ndflvM': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('ndflvM', 'AK4 Number of deepFlavor Medium Jets', 6, -0.5, 5.5)),\n",
    "            'njets': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('njets', 'AK4 Number of Jets', 7, -0.5, 6.5)),\n",
    "\n",
    "            'nfatjets': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('nfatjets', 'AK15 Number of Jets', 7, -0.5, 6.5)),\n",
    "\n",
    "            'TvsQCD': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('TvsQCD', 'TvsQCD', 15, 0., 1)),\n",
    "\n",
    "            'ndcsvM': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('ndcsvM', 'AK4 Number of deepCSV Medium Jets', 6, -0.5, 5.5)),\n",
    "            \n",
    "            'dphi_Met_LJ': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('dphi_Met_LJ', '$\\Delta \\Phi (E^T_{miss}, Leading Jet)$', 30, 0, 3.5)),\n",
    "            'dr_e_lj': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('dr_e_lj', '$\\Delta r (Leading e, Leading Jet)$', 30, 0, 5.0)),\n",
    "            'dr_mu_lj': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('dr_mu_lj', '$\\Delta r (Leading \\mu, Leading Jet)$', 30, 0, 5.0)),\n",
    "            'ele_eta': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('ele_eta', 'Leading Electron Eta', 48, -2.4, 2.4)),\n",
    "            'mu_eta': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('mu_eta', 'Leading Muon Eta', 48, -2.4, 2.4)),\n",
    "            'ele_phi': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('ele_phi', 'Leading Electron Phi', 64, -3.2, 3.2)),\n",
    "            'metphi': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('metphi','MET phi',35,-3.5,3.5)),\n",
    "            'mu_phi': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                hist.Bin('mu_phi', 'Leading Muon Phi', 64, -3.2, 3.2)),\n",
    "            'cutflow': hist.Hist(\n",
    "                'Events',\n",
    "                hist.Cat('dataset', 'Dataset'),\n",
    "                hist.Cat('region', 'Region'),\n",
    "                #hist.Bin('cut', 'Cut index', 11, 0, 11),\n",
    "                hist.Bin('cut', 'Cut index', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17])),\n",
    " \n",
    "        })\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    @property\n",
    "    def fields(self):\n",
    "        return self._fields\n",
    "\n",
    "    def process(self, events):\n",
    "\n",
    "        dataset = events.metadata['dataset']\n",
    "            \n",
    "        selected_regions = []\n",
    "        for region, samples in self._samples.items():\n",
    "            for sample in samples:\n",
    "                if sample not in dataset:\n",
    "                    continue\n",
    "                selected_regions.append(region)\n",
    "\n",
    "        isData = 'genWeight' not in events.fields\n",
    "        selection = processor.PackedSelection()\n",
    "        hout = self.accumulator.identity()\n",
    "\n",
    "        ###\n",
    "        # Getting corrections, ids from .coffea files\n",
    "        ###\n",
    "        if (\"preVFP\" in dataset) and (self._year == '2016'):\n",
    "            get_ele_loose_id_sf = self._corrections['get_ele_tight_id_sf_preVFP'][self._year]\n",
    "            get_ele_tight_id_sf = self._corrections['get_ele_tight_id_sf_preVFP'][self._year]\n",
    "            \n",
    "            get_ele_reco_sf = self._corrections['get_ele_reco_sf_preVFP_above20'][self._year]\n",
    "            get_ele_reco_err = self._corrections['get_ele_reco_err_preVFP_above20'][self._year]\n",
    "            \n",
    "            get_ele_reco_lowet_sf = self._corrections['get_ele_reco_sf_preVFP_below20'][self._year]\n",
    "            get_ele_reco_lowet_err = self._corrections['get_ele_reco_err_preVFP_below20'][self._year]\n",
    "            \n",
    "            get_mu_tight_id_sf = self._corrections['get_mu_tight_id_sf_preVFP'][self._year]\n",
    "            get_mu_loose_id_sf = self._corrections['get_mu_loose_id_sf_preVFP'][self._year]\n",
    "            get_mu_tight_id_err = self._corrections['get_mu_tight_id_err_preVFP'][self._year]\n",
    "            get_mu_loose_err_sf = self._corrections['get_mu_loose_id_err_preVFP'][self._year]            \n",
    "            \n",
    "            \n",
    "            get_mu_tight_iso_sf = self._corrections['get_mu_tight_iso_sf_preVFP'][self._year]\n",
    "            get_mu_loose_iso_sf = self._corrections['get_mu_loose_iso_sf_preVFP'][self._year]\n",
    "            get_mu_tight_iso_err = self._corrections['get_mu_tight_iso_err_preVFP'][self._year]\n",
    "            get_mu_loose_iso_err = self._corrections['get_mu_loose_iso_err_preVFP'][self._year]            \n",
    "\n",
    "            \n",
    "            get_mu_trig_weight = self._corrections['get_mu_trig_weight_preVFP'][self._year]\n",
    "            get_mu_trig_err = self._corrections['get_mu_trig_weight_preVFP'][self._year]\n",
    "            get_ele_loose_id_err = self._corrections['get_ele_loose_id_err_preVFP'][self._year]\n",
    "            get_ele_tight_id_err = self._corrections['get_ele_tight_id_err_preVFP'][self._year]\n",
    "            get_mu_loose_id_err = self._corrections['get_mu_loose_id_err_preVFP'][self._year]\n",
    "            get_mu_tight_id_err = self._corrections['get_mu_tight_id_err_preVFP'][self._year]\n",
    "            \n",
    "            get_deepflav_weight = self._corrections['get_btag_weight_preVFP']['deepflav'][self._year]\n",
    "            \n",
    "        elif (\"postVFP\" in dataset) and (self._year == '2016'):\n",
    "            get_ele_loose_id_sf = self._corrections['get_ele_tight_id_sf_postVFP'][self._year]\n",
    "            get_ele_tight_id_sf = self._corrections['get_ele_tight_id_sf_postVFP'][self._year]\n",
    "            \n",
    "            get_ele_reco_sf = self._corrections['get_ele_reco_sf_postVFP_above20'][self._year]\n",
    "            get_ele_reco_err = self._corrections['get_ele_reco_err_postVFP_above20'][self._year]\n",
    "            \n",
    "            get_ele_reco_lowet_sf = self._corrections['get_ele_reco_sf_postVFP_below20'][self._year]\n",
    "            get_ele_reco_lowet_err = self._corrections['get_ele_reco_err_postVFP_below20'][self._year]\n",
    "            \n",
    "            \n",
    "            get_mu_tight_id_sf = self._corrections['get_mu_tight_id_sf_postVFP'][self._year]\n",
    "            get_mu_loose_id_sf = self._corrections['get_mu_loose_id_sf_postVFP'][self._year]\n",
    "            get_mu_tight_id_err = self._corrections['get_mu_tight_id_err_postVFP'][self._year]\n",
    "            get_mu_loose_id_err = self._corrections['get_mu_loose_id_err_postVFP'][self._year]            \n",
    "            \n",
    "            get_mu_tight_iso_sf = self._corrections['get_mu_tight_iso_sf_postVFP'][self._year]\n",
    "            get_mu_loose_iso_sf = self._corrections['get_mu_loose_iso_sf_postVFP'][self._year]\n",
    "            get_mu_tight_iso_err = self._corrections['get_mu_tight_iso_err_postVFP'][self._year]\n",
    "            get_mu_loose_iso_err = self._corrections['get_mu_loose_iso_err_postVFP'][self._year]\n",
    "            \n",
    "            \n",
    "            get_mu_trig_weight = self._corrections['get_mu_trig_weight_postVFP'][self._year]\n",
    "            get_mu_trig_err = self._corrections['get_mu_trig_weight_postVFP'][self._year]\n",
    "            get_ele_loose_id_err = self._corrections['get_ele_loose_id_err_postVFP'][self._year]\n",
    "            get_ele_tight_id_err = self._corrections['get_ele_tight_id_err_postVFP'][self._year]\n",
    "            get_mu_loose_id_err = self._corrections['get_mu_loose_id_err_postVFP'][self._year]\n",
    "            get_mu_tight_id_err = self._corrections['get_mu_tight_id_err_postVFP'][self._year]\n",
    "            \n",
    "            get_deepflav_weight = self._corrections['get_btag_weight_postVFP']['deepflav'][self._year]\n",
    "\n",
    "        else:\n",
    "#             print(\"hi\")\n",
    "            get_ele_loose_id_sf = self._corrections['get_ele_tight_id_sf_postVFP'][self._year]\n",
    "            get_ele_tight_id_sf = self._corrections['get_ele_tight_id_sf_postVFP'][self._year]\n",
    "            \n",
    "            get_ele_reco_sf = self._corrections['get_ele_reco_sf_postVFP_above20'][self._year]\n",
    "            get_ele_reco_err = self._corrections['get_ele_reco_err_postVFP_above20'][self._year]\n",
    "            \n",
    "            get_ele_reco_lowet_sf = self._corrections['get_ele_reco_sf_postVFP_below20'][self._year]\n",
    "            get_ele_reco_lowet_err = self._corrections['get_ele_reco_err_postVFP_below20'][self._year]\n",
    "            \n",
    "            \n",
    "            get_mu_tight_id_sf = self._corrections['get_mu_tight_id_sf_postVFP'][self._year]\n",
    "            get_mu_loose_id_sf = self._corrections['get_mu_loose_id_sf_postVFP'][self._year]\n",
    "            get_mu_tight_id_err = self._corrections['get_mu_tight_id_err_postVFP'][self._year]\n",
    "            get_mu_loose_id_err = self._corrections['get_mu_loose_id_err_postVFP'][self._year]            \n",
    "            \n",
    "            get_mu_tight_iso_sf = self._corrections['get_mu_tight_iso_sf_postVFP'][self._year]\n",
    "            get_mu_loose_iso_sf = self._corrections['get_mu_loose_iso_sf_postVFP'][self._year]\n",
    "            get_mu_tight_iso_err = self._corrections['get_mu_tight_iso_err_postVFP'][self._year]\n",
    "            get_mu_loose_iso_err = self._corrections['get_mu_loose_iso_err_postVFP'][self._year]\n",
    "            \n",
    "            \n",
    "            get_mu_trig_weight = self._corrections['get_mu_trig_weight_postVFP'][self._year]\n",
    "            get_mu_trig_err = self._corrections['get_mu_trig_weight_postVFP'][self._year]\n",
    "            get_ele_loose_id_err = self._corrections['get_ele_loose_id_err_postVFP'][self._year]\n",
    "            get_ele_tight_id_err = self._corrections['get_ele_tight_id_err_postVFP'][self._year]\n",
    "            get_mu_loose_id_err = self._corrections['get_mu_loose_id_err_postVFP'][self._year]\n",
    "            get_mu_tight_id_err = self._corrections['get_mu_tight_id_err_postVFP'][self._year]\n",
    "            \n",
    "            get_deepflav_weight = self._corrections['get_btag_weight_postVFP']['deepflav'][self._year]\n",
    "\n",
    "        get_msd_weight = self._corrections['get_msd_weight']\n",
    "        get_ttbar_weight = self._corrections['get_ttbar_weight']\n",
    "        get_nnlo_nlo_weight = self._corrections['get_nnlo_nlo_weight'][self._year]\n",
    "        get_nlo_qcd_weight = self._corrections['get_nlo_qcd_weight'][self._year]\n",
    "        get_nlo_ewk_weight = self._corrections['get_nlo_ewk_weight'][self._year]\n",
    "        get_pu_weight = self._corrections['get_pu_weight'][self._year]\n",
    "        get_met_trig_weight = self._corrections['get_met_trig_weight'][self._year]\n",
    "#         get_met_zmm_trig_weight = self._corrections['get_met_zmm_trig_weight'][self._year]\n",
    "        get_ele_trig_weight = self._corrections['get_ele_trig_weight'][self._year]\n",
    "        get_ele_trig_err    = self._corrections['get_ele_trig_err'][self._year]\n",
    "#         get_mu_trig_weight = self._corrections['get_mu_trig_weight'][self._year]\n",
    "        get_pho_trig_weight = self._corrections['get_pho_trig_weight'][self._year]\n",
    "#         get_ele_loose_id_sf = self._corrections['get_ele_loose_id_sf'][self._year]\n",
    "#         get_ele_tight_id_sf = self._corrections['get_ele_tight_id_sf'][self._year]\n",
    "        get_pho_tight_id_sf = self._corrections['get_pho_tight_id_sf'][self._year]\n",
    "        get_pho_csev_sf = self._corrections['get_pho_csev_sf'][self._year]\n",
    "#         get_mu_tight_id_sf = self._corrections['get_mu_tight_id_sf'][self._year]\n",
    "#         get_mu_loose_id_sf = self._corrections['get_mu_loose_id_sf'][self._year]\n",
    "#         get_ele_reco_sf = self._corrections['get_ele_reco_sf'][self._year]\n",
    "#         get_ele_reco_lowet_sf = self._corrections['get_ele_reco_lowet_sf']\n",
    "#         get_mu_tight_iso_sf = self._corrections['get_mu_tight_iso_sf'][self._year]\n",
    "#         get_mu_loose_iso_sf = self._corrections['get_mu_loose_iso_sf'][self._year]\n",
    "        get_ecal_bad_calib = self._corrections['get_ecal_bad_calib']\n",
    "#         get_deepflav_weight = self._corrections['get_btag_weight']['deepflav'][self._year]\n",
    "#         get_deepcsv_weight = self._corrections['get_btag_weight']['deepcsv'][self._year]\n",
    "#         Jetevaluator = self._corrections['Jetevaluator']\n",
    "\n",
    "        isLooseElectron = self._ids['isLooseElectron']\n",
    "        isTightElectron = self._ids['isTightElectron']\n",
    "        isLooseMuon = self._ids['isLooseMuon']\n",
    "        isTightMuon = self._ids['isTightMuon']\n",
    "        isLooseTau = self._ids['isLooseTau']\n",
    "        isLoosePhoton = self._ids['isLoosePhoton']\n",
    "        isTightPhoton = self._ids['isTightPhoton']\n",
    "        isGoodJet = self._ids['isGoodJet']\n",
    "        isGoodFatJet = self._ids['isGoodFatJet']\n",
    "        isHEMJet = self._ids['isHEMJet']\n",
    "\n",
    "        match = self._common['match']\n",
    "        # to calculate photon trigger efficiency\n",
    "        sigmoid = self._common['sigmoid']\n",
    "        deepflavWPs = self._common['btagWPs']['deepflav'][self._year]\n",
    "        deepcsvWPs = self._common['btagWPs']['deepcsv'][self._year]\n",
    "\n",
    "        ###\n",
    "        # Derive jet corrector for JEC/JER\n",
    "        ###\n",
    "\n",
    "#         JECcorrector = FactorizedJetCorrector(**{name: Jetevaluator[name] for name in self._jec[self._year]})\n",
    "#         JECuncertainties = JetCorrectionUncertainty(**{name: Jetevaluator[name] for name in self._junc[self._year]})\n",
    "#         JER = JetResolution(**{name: Jetevaluator[name] for name in self._jr[self._year]})\n",
    "#         JERsf = JetResolutionScaleFactor(**{name: Jetevaluator[name] for name in self._jersf[self._year]})\n",
    "#         Jet_transformer = JetTransformer(jec=JECcorrector, junc=JECuncertainties, jer=JER, jersf=JERsf)\n",
    "\n",
    "        ###\n",
    "        # Initialize global quantities (MET ecc.)\n",
    "        ###\n",
    "\n",
    "        met = events.MET\n",
    "        met[\"T\"] = ak.zip({\"pt\": met.pt, \"phi\": met.phi}, \n",
    "                          with_name=\"PolarTwoVector\", \n",
    "                          behavior=vector.behavior)\n",
    "        calomet = events.CaloMET\n",
    "        puppimet = events.PuppiMET\n",
    "        puppimet[\"T\"] = ak.zip({\"pt\": puppimet.pt, \"phi\": puppimet.phi}, \n",
    "                          with_name=\"PolarTwoVector\", \n",
    "                          behavior=vector.behavior)\n",
    "\n",
    "        ###\n",
    "        # Initialize physics objects\n",
    "        ###\n",
    "\n",
    "        mu = events.Muon\n",
    "        mu['isloose'] = isLooseMuon(mu.pt, mu.eta, mu.pfRelIso04_all, mu.looseId, self._year)\n",
    "        mu['istight'] = isTightMuon(mu.pt, mu.eta, mu.pfRelIso04_all, mu.tightId, self._year)\n",
    "        mu[\"T\"] = ak.zip({\"pt\": mu.pt, \"phi\": mu.phi}, \n",
    "                  with_name=\"PolarTwoVector\", \n",
    "                  behavior=vector.behavior)\n",
    "        mu['p4'] = ak.zip({\n",
    "                            \"pt\": mu.pt,\n",
    "                            \"eta\": mu.eta,\n",
    "                            \"phi\": mu.phi,\n",
    "                            \"mass\": mu.mass},\n",
    "                            with_name=\"PtEtaPhiMLorentzVector\",\n",
    "        )\n",
    "    \n",
    "\n",
    "        mu_loose = mu[ak.values_astype(mu.isloose, np.bool)]\n",
    "        mu_tight = mu[ak.values_astype(mu.istight, np.bool)]\n",
    "        ak.num(mu, axis=1)\n",
    "        mu_ntot = ak.num(mu, axis=1)\n",
    "        mu_nloose = ak.num(mu_loose, axis=1)\n",
    "        mu_ntight = ak.num(mu_tight, axis=1)\n",
    "        leading_mu = mu_tight[:,:1]\n",
    "        leading_mu[\"T\"] = ak.zip({\"pt\": leading_mu.pt, \"phi\": leading_mu.phi},\n",
    "                with_name = \"PolarTwoVector\",\n",
    "                behavior=vector.behavior)\n",
    "        \n",
    "        \n",
    "        e = events.Electron\n",
    "        event_size = len(events)\n",
    "        \n",
    "        \n",
    "        e['isclean'] = ak.all(e.metric_table(mu_loose) > 0.3, axis=-1)\n",
    "        e['isloose'] = isLooseElectron(e.pt, e.eta+e.deltaEtaSC, e.dxy, e.dz, e.cutBased, self._year)\n",
    "        e['istight'] = isTightElectron(e.pt, e.eta+e.deltaEtaSC, e.dxy, e.dz, e.cutBased, self._year)\n",
    "        e[\"T\"] = ak.zip({\"pt\": e.pt, \"phi\": e.phi}, \n",
    "                  with_name=\"PolarTwoVector\", \n",
    "                  behavior=vector.behavior)\n",
    "        e['p4'] = ak.zip({\n",
    "                            \"pt\": e.pt,\n",
    "                            \"eta\": e.eta,\n",
    "                            \"phi\": e.phi,\n",
    "                            \"mass\": e.mass},\n",
    "                            with_name=\"PtEtaPhiMLorentzVector\",\n",
    "        )\n",
    "        e_clean = e[ak.values_astype(e.isclean, np.bool)]\n",
    "        e_loose = e_clean[ak.values_astype(e_clean.isloose, np.bool)]\n",
    "        e_tight = e_clean[ak.values_astype(e_clean.istight, np.bool)]\n",
    "        e_ntot = ak.num(e, axis=1)\n",
    "        e_nloose = ak.num(e_loose, axis=1)\n",
    "\n",
    "        e_ntight = ak.num(e_tight, axis=1)\n",
    "        leading_e = e_tight[:,:1]\n",
    "        \n",
    "        tau = events.Tau\n",
    "        tau['isclean'] = ak.all(tau.metric_table(mu_loose) > 0.4, axis=-1) & ak.all(tau.metric_table(e_loose) > 0.4, axis=-1)\n",
    "#         ak.all(tau.metric_table(mu_loose) > 0.4, axis=-1)\n",
    "        try:\n",
    "            tau['isloose']=isLooseTau(tau.pt,tau.eta,tau.idDecayMode,tau.idDeepTau2017v2p1VSjet,self._year)\n",
    "        except:\n",
    "            tau['isloose']=isLooseTau(tau.pt,tau.eta,tau.idDecayModeOldDMs,tau.idDeepTau2017v2p1VSjet,self._year)\n",
    "        else: \n",
    "            tau['isloose']=isLooseTau(tau.pt,tau.eta,tau.idDecayModeNewDMs,tau.idDeepTau2017v2p1VSjet,self._year)\n",
    "\n",
    "        tau_clean = tau[ak.values_astype(tau.isclean, np.bool)]\n",
    "\n",
    "        tau_loose = tau_clean[ak.values_astype(tau_clean.isloose, np.bool)]\n",
    "        tau_ntot = ak.num(tau, axis=1)\n",
    "        tau_nloose = ak.num(tau_loose, axis=1)\n",
    "\n",
    "        pho = events.Photon\n",
    "        pho['isclean'] = ak.all(pho.metric_table(mu_loose) > 0.4, axis=-1) & ak.all(pho.metric_table(e_loose) > 0.4, axis=-1)\n",
    "        _id = 'cutBased'\n",
    "        if self._year == '2016':\n",
    "            _id = 'cutBased'\n",
    "        pho['isloose'] = isLoosePhoton(pho.pt, pho.eta, pho[_id], self._year) & (pho.electronVeto)  # added electron veto flag\n",
    "        pho['istight'] = isTightPhoton(pho.pt, pho[_id], self._year) & (pho.isScEtaEB) & (pho.electronVeto)  # tight photons are barrel only\n",
    "\n",
    "        pho[\"T\"] = ak.zip({\"pt\": pho.pt, \"phi\": pho.phi}, \n",
    "                  with_name=\"PolarTwoVector\", \n",
    "                  behavior=vector.behavior)\n",
    "    \n",
    "        pho_clean = pho[ak.values_astype(pho.isclean, np.bool)]\n",
    "        pho_loose = pho_clean[ak.values_astype(pho_clean.isloose, np.bool)]\n",
    "        pho_tight = pho_clean[ak.values_astype(pho_clean.istight, np.bool)]\n",
    "        pho_ntot = ak.num(pho,axis=1)\n",
    "        pho_nloose = ak.num(pho_loose, axis=1)\n",
    "        pho_ntight = ak.num(pho_tight, axis=1)\n",
    "        leading_pho = pho[:,:1] #new way to define leading photon\n",
    "        leading_pho = leading_pho[ak.values_astype(leading_pho.isclean, np.bool)]\n",
    "        \n",
    "        leading_pho = leading_pho[ak.values_astype(leading_pho.istight, np.bool)]\n",
    "        leading_pho = leading_pho[ak.values_astype(leading_pho.istight, np.bool)]\n",
    "\n",
    "        fj = events.AK15PFPuppi\n",
    "        fj['pt'] = events.AK15PFPuppi['Jet_pt']\n",
    "        fj['phi'] = events.AK15PFPuppi['Jet_phi']\n",
    "        fj['eta'] = events.AK15PFPuppi['Jet_eta']\n",
    "        fj['mass'] = events.AK15PFPuppi['Jet_mass']\n",
    "        #fj['TvsQCD'] = events.AK15PFPuppi['Jet_particleNetAK15_TvsQCD']\n",
    "        fj[\"T\"] = ak.zip({\"pt\": fj.Jet_pt, \"phi\": fj.Jet_phi},\n",
    "                                with_name=\"PolarTwoVector\",\n",
    "                                behavior=vector.behavior)\n",
    "        fj['p4'] = ak.zip({\n",
    "            \"pt\"  : fj.Jet_pt,\n",
    "            \"eta\" : fj.Jet_eta,\n",
    "            \"phi\" : fj.Jet_phi,\n",
    "            \"mass\": fj.Jet_mass},\n",
    "            with_name=\"PtEtaPhiMCollection\",\n",
    "        )\n",
    "        fj['sd'] = ak.zip({\n",
    "            \"pt\"  : fj.Subjet_pt,\n",
    "            \"phi\" : fj.Subjet_phi,\n",
    "            \"eta\" : fj.Subjet_eta,\n",
    "            \"mass\": fj.Subjet_mass},\n",
    "            with_name=\"PtEtaPhiMCollection\",\n",
    "        )\n",
    "        fj['probQCDb'] = events.AK15PFPuppi['Jet_particleNetAK15_QCDb']\n",
    "        fj['probQCDbb'] = events.AK15PFPuppi['Jet_particleNetAK15_QCDbb']\n",
    "        fj['probQCDc'] = events.AK15PFPuppi['Jet_particleNetAK15_QCDc']\n",
    "        fj['probQCDcc'] = events.AK15PFPuppi['Jet_particleNetAK15_QCDcc']\n",
    "        fj['probTbcq'] = events.AK15PFPuppi['Jet_particleNetAK15_Tbcq']\n",
    "        fj['probTbqq'] = events.AK15PFPuppi['Jet_particleNetAK15_Tbqq']\n",
    "        fj['probQCDothers'] = events.AK15PFPuppi['Jet_particleNetAK15_QCDothers']\n",
    "        probQCD=fj.probQCDbb+fj.probQCDcc+fj.probQCDb+fj.probQCDc+fj.probQCDothers\n",
    "        probT=fj.probTbcq+fj.probTbqq\n",
    "        fj['TvsQCD'] = probT/(probT+probQCD)\n",
    "\n",
    "        fjEleMask = ak.all(fj.p4.metric_table(e_loose) > 1.5, axis=-1)\n",
    "        fjMuMask = ak.all(fj.p4.metric_table(mu_loose) > 1.5, axis=-1)\n",
    "        fjPhoMask = ak.all(fj.p4.metric_table(pho_loose) > 1.5, axis=-1)\n",
    "\n",
    "        fj_isclean_mask = (fjMuMask & fjEleMask & fjPhoMask)\n",
    "        fj_isgood_mask = isGoodFatJet(fj.pt, fj.eta, fj.Jet_jetId)\n",
    "        fj_good_clean = fj[fj_isclean_mask & fj_isgood_mask]\n",
    "        fj_clean = fj[fj_isclean_mask]\n",
    "        fj_nclean = ak.num(fj_clean)\n",
    "\n",
    "        leading_fj = fj_good_clean[:,:1]\n",
    "\n",
    "        j = events.Jet\n",
    "        j['isgood'] = isGoodJet(j.pt, j.eta, j.jetId, j.puId, j.neHEF, j.chHEF, self._year)\n",
    "        j['isHEM'] = isHEMJet(j.pt, j.eta, j.phi)\n",
    "        j['isdcsvL'] = (j.btagDeepB>deepcsvWPs['loose'])\n",
    "        j['isdflvL'] = (j.btagDeepFlavB>deepflavWPs['loose'])\n",
    "        j['isdflvM'] = (j.btagDeepFlavB > deepflavWPs['medium']) ## from Rishab\n",
    "        j['isdcsvM'] = (j.btagDeepB > deepcsvWPs['medium']) ## from Rishabh\n",
    "\n",
    "        j[\"T\"] = ak.zip({\"pt\": j.pt, \"phi\": j.phi}, \n",
    "                  with_name=\"PolarTwoVector\", \n",
    "                  behavior=vector.behavior)\n",
    "        j['p4'] = ak.zip({\n",
    "        \"pt\": j.pt,\n",
    "        \"eta\": j.eta,\n",
    "        \"phi\": j.phi,\n",
    "        \"mass\": j.mass},\n",
    "        with_name=\"PtEtaPhiMLorentzVector\",\n",
    "        )\n",
    "        \n",
    "#         https://coffeateam.github.io/coffea/modules/coffea.nanoevents.methods.vector.html#\n",
    "#         j['ptRaw'] =j.pt * (1-j.rawFactor)\n",
    "#         j['massRaw'] = j.mass * (1-j.rawFactor)\n",
    "#         j['rho'] = j.pt.ones_like()*events.fixedGridRhoFastjetAll.array\n",
    "\n",
    "#         j_dflvL = j_clean[j_clean.isdflvL.astype(np.bool)]\n",
    "        jetMuMask = ak.all(j.metric_table(mu_loose) > 0.4, axis=-1)\n",
    "        jetEleMask = ak.all(j.metric_table(e_loose) > 0.4, axis=-1)\n",
    "        jetPhoMask = ak.all(j.metric_table(pho_loose) > 0.4, axis=-1)\n",
    "\n",
    "        j_isclean_mask = (jetMuMask & jetEleMask & jetPhoMask)\n",
    "        j_isgood_mask = isGoodJet(j.pt, j.eta, j.jetId, j.puId, j.neHEF, j.chHEF, self._year)\n",
    "        j_good_clean = j[j_isclean_mask & j_isgood_mask]\n",
    "        j_ngood_clean = ak.num(j_good_clean)\n",
    "        j_good_clean_dflvB = j_good_clean.isdflvM\n",
    "        j_ndflvM = ak.num(j[j_good_clean_dflvB])\n",
    "        leading_j = j_good_clean[:,:1] # new way to define leading jet\n",
    "        j_HEM = j[ak.values_astype(j.isHEM, np.bool)]       \n",
    "        j_nHEM = ak.num(j_HEM, axis=1)\n",
    "        atleast_one_jet_with_pt_grt_50 = ((ak.num(j_good_clean)>=1) & ak.any(j_good_clean.pt>=50, axis=-1))\n",
    "        # *****btag\n",
    "        # https://twiki.cern.ch/twiki/bin/viewauth/CMS/BtagRecommendation102X#Supported_Algorithms_and_Operati\n",
    "        # medium     0.4184\n",
    "#         btagWP_medium = 0.4184\n",
    "#         Jet_btag_medium = j_clean[j_clean['btagDeepB'] > btagWP_medium]\n",
    "        ###\n",
    "        # Calculating derivatives\n",
    "        ###\n",
    "\n",
    "        # ************ calculate delta phi( leading ak4jet, met) > 1.5***********\n",
    "\n",
    "        met_LJ_Pairs = ak.cartesian({\"met\":met, \"lj\": leading_j})\n",
    "        Delta_Phi_Met_LJ_var = (abs(met_LJ_Pairs.met.delta_phi(met_LJ_Pairs.lj)))\n",
    "        Delta_Phi_Met_LJ = (abs(met_LJ_Pairs.met.delta_phi(met_LJ_Pairs.lj))>1.5)\n",
    "\n",
    "        # *******calculate deltaR( leading ak4jet, e/mu) < 3.4 *****\n",
    "        LJ_Ele = ak.cartesian({\"leading_j\":leading_j, \"e_loose\": e_loose})\n",
    "        DeltaR_LJ_Ele = abs(LJ_Ele.leading_j.delta_r(LJ_Ele.e_loose))\n",
    "        \n",
    "        DeltaR_LJ_Ele_mask = ak.any(DeltaR_LJ_Ele < 3.4, axis=-1)\n",
    "\n",
    "        LJ_Mu = ak.cartesian({\"leading_j\":leading_j, \"mu_loose\": mu_loose})\n",
    "        DeltaR_LJ_Mu = abs(LJ_Mu.leading_j.delta_r(LJ_Mu.mu_loose))\n",
    "        \n",
    "        DeltaR_LJ_Mu_mask = ak.any(DeltaR_LJ_Mu < 3.4, axis=-1)\n",
    "#         ele_pairs = e_loose.distincts()\n",
    "#         diele = ele_pairs.i0+ele_pairs.i1\n",
    "#         diele['T'] = TVector2Array.from_polar(diele.pt, diele.phi)\n",
    "#         leading_ele_pair = ele_pairs[diele.pt.argmax()]\n",
    "#         leading_diele = diele[diele.pt.argmax()]\n",
    "\n",
    "#         mu_pairs = mu_loose.distincts()\n",
    "#         dimu = mu_pairs.i0+mu_pairs.i1\n",
    "# #         dimu['T'] = TVector2Array.from_polar(dimu.pt, dimu.phi)\n",
    "#         leading_mu_pair = mu_pairs[dimu.pt.argmax()]\n",
    "#         leading_dimu = dimu[dimu.pt.argmax()]\n",
    "\n",
    "        ###\n",
    "        # Calculate Recoil\n",
    "        ###\n",
    "        recoil_t = ak.zip({\"pt\": met.pt+leading_e.pt, \"phi\": met.phi+leading_e.phi}, \n",
    "                  with_name=\"PolarTwoVector\", \n",
    "                  behavior=vector.behavior)\n",
    "        mete = met + leading_e\n",
    "        metm = met + leading_mu\n",
    "        #print('mete: ', mete)\n",
    "        mete['T'] = ak.zip({\"pt\": mete.pt, \"phi\": mete.phi},\n",
    "                with_name=\"PolarTwoVector\",\n",
    "                behavior=vector.behavior)\n",
    "        #recoil_t = met.T+leading_e.T\n",
    "        #print('test recoil: ', recoil_t)\n",
    "\n",
    "        u = { # recoil\n",
    "            'sr': met.T,\n",
    "            'wmcr': metm,\n",
    "            'tmcr': metm,\n",
    "            'wecr': mete,\n",
    "            'tecr': mete,\n",
    "#            'zmcr': met.T+leading_mu.T,\n",
    "#            'zecr': met.T+leading_e.T,\n",
    "#            'gcr': met.T+leading_pho.T\n",
    "        }\n",
    "\n",
    "        mT = {\n",
    "            'sr': np.sqrt(2*leading_e.pt*met.pt*(1-np.cos(met.delta_phi(leading_e)))),\n",
    "            'wmcr': np.sqrt(2*leading_mu.pt*met.pt*(1-np.cos(met.delta_phi(leading_mu)))),\n",
    "            'tmcr': np.sqrt(2*leading_mu.pt*met.pt*(1-np.cos(met.delta_phi(leading_mu)))),\n",
    "            'wecr': np.sqrt(2*leading_e.pt*met.pt*(1-np.cos(met.delta_phi(leading_e)))),\n",
    "            'tecr': np.sqrt(2*leading_e.pt*met.pt*(1-np.cos(met.delta_phi(leading_e)))),\n",
    "            'zmcr': np.sqrt(2*leading_mu.pt*met.pt*(1-np.cos(met.delta_phi(leading_mu)))),\n",
    "            'zecr': np.sqrt(2*leading_e.pt*met.pt*(1-np.cos(met.delta_phi(leading_e)))),\n",
    "            'gcr': np.sqrt(2*leading_mu.pt*met.pt*(1-np.cos(met.delta_phi(leading_mu))))\n",
    "        }\n",
    "\n",
    "\n",
    "        ###\n",
    "        # Calculating weights\n",
    "        ###\n",
    "        if not isData:\n",
    "\n",
    "            ###\n",
    "            # JEC/JER\n",
    "            ###\n",
    "\n",
    "            gen = events.GenPart\n",
    "            ###\n",
    "            # Fat-jet top matching at decay level\n",
    "            ###\n",
    "            qFromW = gen[\n",
    "                (abs(gen.pdgId) < 4) &\n",
    "                gen.hasFlags(['fromHardProcess', 'isFirstCopy']) &\n",
    "                (abs(gen.distinctParent.pdgId) == 24)\n",
    "            ]\n",
    "            cFromW = gen[\n",
    "                (abs(gen.pdgId) == 4) &\n",
    "                gen.hasFlags(['fromHardProcess', 'isFirstCopy']) &\n",
    "                (abs(gen.distinctParent.pdgId) == 24)\n",
    "            ]\n",
    "            bFromTop = gen[(\n",
    "                abs(gen.pdgId) == 5) & \n",
    "                gen.hasFlags(['fromHardProcess','isFirstCopy']) & \n",
    "                (gen.distinctParent.pdgId == 6)\n",
    "            ]\n",
    "            qFromWFromTop = qFromW[qFromW.distinctParent.distinctParent.pdgId == 6]\n",
    "            qFromWFromTop['p4'] = ak.zip({\n",
    "                \"pt\"  : qFromWFromTop.pt,\n",
    "                \"eta\" : qFromWFromTop.eta,\n",
    "                \"phi\" : qFromWFromTop.phi,\n",
    "                \"mass\": qFromWFromTop.mass}, \n",
    "                with_name=\"PtEtaPhiMCollection\",)\n",
    "            print('qWT.pt, phi: ', qFromWFromTop.pt, qFromWFromTop.phi)\n",
    "            jetgenWq = ak.cartesian({'fj': fj.sd, 'qWT': qFromWFromTop.p4})\n",
    "#            dr_jetgenWq = jetgenWq.fj.delta_r(jetgenWq.qWT)\n",
    "            #print('jetgenWq0: ', jetgenWq[0])\n",
    "\n",
    "#            def tbqqmatch(topid, dR=1.5):\n",
    "#                qFromWFromTop = qFromW[qFromW.distinctParent.distinctParent.pdgId == topid]\n",
    "#                bFromTop = gen[\n",
    "#                    (abs(gen.pdgId) == 5) &\n",
    "#                    gen.hasFlags(['fromHardProcess','isFirstCopy']) &\n",
    "#                    (gen.distinctParent.pdgId == topid)\n",
    "#                ]\n",
    "#                jetgenWq = fj.sd.cross(qFromWFromTop, nested=True)\n",
    "            print('gen.pdgId: ', abs(gen.pdgId))\n",
    "            print('hasFlags: ', gen[gen.hasFlags(['fromHardProcess', 'isFirstCopy'])])\n",
    "            print('distinctparent: ', gen.distinctParent.pdgId)\n",
    "            print('qFromW: ', qFromW)\n",
    "            gen['isb'] = (abs(gen.pdgId) == 5) & gen.hasFlags(['fromHardProcess', 'isLastCopy'])\n",
    "\n",
    "            gen['isc'] = (abs(gen.pdgId) == 4) & gen.hasFlags(['fromHardProcess', 'isLastCopy'])\n",
    "\n",
    "            gen['isTop'] = (abs(gen.pdgId) == 6) & gen.hasFlags(['fromHardProcess', 'isLastCopy'])\n",
    "            genTops = gen[gen.isTop]\n",
    "            ttjet_weights = np.ones(event_size)\n",
    "            if('TTJets' in dataset):\n",
    "                ttjet_weights = np.sqrt(get_ttbar_weight(genTops.pt.sum()) * get_ttbar_weight(genTops.pt.sum()))\n",
    "\n",
    "            gen['isW'] = (abs(gen.pdgId) == 24) & gen.hasFlags(['fromHardProcess', 'isLastCopy'])\n",
    "            gen['isZ'] = (abs(gen.pdgId) == 23) & gen.hasFlags(['fromHardProcess', 'isLastCopy'])\n",
    "            gen['isA'] = (abs(gen.pdgId) == 22) & gen.hasFlags(['isPrompt', 'fromHardProcess', 'isLastCopy']) & (gen.status == 1)\n",
    "\n",
    "            ###\n",
    "            # Calculating gen photon dynamic isolation as in https://arxiv.org/pdf/1705.04664.pdf\n",
    "            ###\n",
    "                \n",
    "            epsilon_0_dyn = 0.1\n",
    "            n_dyn = 1\n",
    "            gen['R_dyn'] = (91.1876/(gen.pt * np.sqrt(epsilon_0_dyn))) * ak.values_astype((gen.isA), np.int) + (-999)*ak.values_astype((~gen.isA), np.int)\n",
    "            gen['R_0_dyn'] = gen.R_dyn * ak.values_astype((gen.R_dyn < 1.0), np.int) + ak.values_astype((gen.R_dyn >= 1.0), np.int)   \n",
    "            print(\"gen[R_dyn]: \", gen.R_dyn)\n",
    "            print(\"gen[R_0_dyn]: \", gen['R_0_dyn'])\n",
    "\n",
    "            def isolation(R):\n",
    "                hadrons = gen[  # Stable hadrons not in NanoAOD, using quarks/glouns instead\n",
    "                    ((abs(gen.pdgId) <= 5) | (abs(gen.pdgId) == 21)) & \n",
    "                    gen.hasFlags(['fromHardProcess', 'isFirstCopy'])\n",
    "                ]    \n",
    "                genhadrons = ak.cartesian({\"gen\": gen, \"hadrons\": hadrons})\n",
    "                #genhadrons = gen.cross(hadrons, nested=True)\n",
    "                #print(\"genhadrons.gen fields\", genhadrons.gen.fields)\n",
    "                #print(\"genhadrons.hadrons fields\", genhadrons.hadrons.fields)\n",
    "                #print(genhadrons.gen.delta_r(genhadrons.hadrons))\n",
    "                #print(\"R\", R)\n",
    "                #import sys\n",
    "                #sys.exit(0)\n",
    "                dR_gen_had = abs(genhadrons.gen.delta_r(genhadrons.hadrons))\n",
    "                R_dR = ak.cartesian({\"R\": R, \"dR\": dR_gen_had})\n",
    "                print('dr0', genhadrons.gen.delta_r(genhadrons.hadrons)[-1], \"len:\", len(genhadrons.gen.delta_r(genhadrons. hadrons)[-1]))\n",
    "                print('r0', R[-1], \"len:\", len(R[-1]))\n",
    "                print('dr0', R_dR.dR[-1], \"len:\", len(R_dR.dR[-1]))\n",
    "                print('r0', R_dR.R[-1], \"len:\", len(R_dR.R[-1]))\n",
    "\n",
    "                #mask_gen_had = abs(genhadrons.gen.delta_r(genhadrons.hadrons)) <=R\n",
    "                mask_gen_had = R_dR.dR <= R_dR.R\n",
    "                print('mask_gen_had:', mask_gen_had)\n",
    "                print('genhadrons.hadrons.pt: ', genhadrons.hadrons.pt)\n",
    "                hadronic_et = (genhadrons.hadrons[mask_gen_had].pt)\n",
    "                print('IsISO-1: ', (1 - np.cos(R)))\n",
    "                print('IsISO-2: ', (1 - np.cos(gen.R_0_dyn)))\n",
    "                IsIso_3 = epsilon_0_dyn * gen.pt * np.power((1 - np.cos(R)) / (1 - np.cos(gen.R_0_dyn)),n_dyn)\n",
    "                print('IsISO-3: ', IsIso_3, 'len: ', len(IsIso_3))\n",
    "                print('hadronic_et: ', hadronic_et, 'len: ', len(hadronic_et))\n",
    "                IsIso_5 = ak.num(hadrons, axis=1) == 0            \n",
    "                print('IsISO-5: ', IsIso_5, 'len: ', len(IsIso_5))\n",
    "                IsIso_4 = hadronic_et <= IsIso_3\n",
    "                print('IsISO-4: ', IsIso_4)\n",
    "                IsISO = (hadronic_et <= (epsilon_0_dyn * gen.pt * np.power((1 - np.cos(R)) / (1 - np.cos(gen.R_0_dyn)), n_dyn))) | (ak.num(hadrons, axis=1) == 0)\n",
    "                print('IsISO: ', IsISO)\n",
    "                return (hadronic_et <= (epsilon_0_dyn * gen.pt * np.power((1 - np.cos(R)) / (1 - np.cos(gen.R_0_dyn)), n_dyn))) | (ak.num(hadrons, axis=1) == 0)            \n",
    "\n",
    "#            def isolation(R):\n",
    "#                hadrons = gen[  # Stable hadrons not in NanoAOD, using quarks/glouns instead\n",
    "#                    ((abs(gen.pdgId) <= 5) | (abs(gen.pdgId) == 21)) & \n",
    "#                    gen.hasFlags(['fromHardProcess', 'isFirstCopy'])\n",
    "#                ]\n",
    "#                genhadrons = ak.cartesian({\"gen\": gen, \"hadrons\": hadrons})\n",
    "#                #genhadrons = gen.cross(hadrons, nested=True)\n",
    "#                print(\"genhadrons.gen fields\", genhadrons.gen.fields)\n",
    "#                print(\"genhadrons.hadrons fields\", genhadrons.hadrons.fields)\n",
    "#                #print(genhadrons.gen.delta_r(genhadrons.hadrons))\n",
    "#                #print(\"R\", R)\n",
    "#                #import sys\n",
    "#                #sys.exit(0)\n",
    "#                mask_gen_had = abs(genhadrons.gen.delta_r(genhadrons.hadrons)) <=R\n",
    "#                hadronic_et = (genhadrons.hadrons[mask_gen_had].pt)\n",
    "#                hadronic_et = genhadrons.i1[(genhadrons.i0.delta_r(genhadrons.i1) <= R)].pt.sum()\n",
    "#                return (hadronic_et <= (epsilon_0_dyn * gen.pt * np.power((1 - np.cos(R)) / (1 - np.cos(gen.R_0_dyn)), n_dyn))) | (ak.num(hadrons, axis=1) == 0)\n",
    "\n",
    "            isIsoA = gen.isA\n",
    "            iterations = 5.\n",
    "            for i in range(1, int(iterations) + 1):\n",
    "                isIsoA = isIsoA & isolation(gen.R_0_dyn*i/iterations)\n",
    "            gen['isIsoA'] = isIsoA\n",
    "#\n",
    "            genWs = gen[gen.isW & (gen.pt > 100)]\n",
    "            genZs = gen[gen.isZ & (gen.pt > 100)]\n",
    "            genDYs = gen[gen.isZ & (gen.mass > 30)]\n",
    "#            # Based on photon weight distribution\n",
    "#            genIsoAs = gen[gen.isIsoA & (gen.pt > 100)]\n",
    "\n",
    "            nnlo_nlo = {}\n",
    "            nlo_qcd = np.ones(event_size)\n",
    "            nlo_ewk = np.ones(event_size)\n",
    "#             if('GJets' in dataset):\n",
    "#                 if self._year == '2016':\n",
    "#                     nlo_qcd = get_nlo_qcd_weight['a'](genIsoAs.pt.max())\n",
    "#                     nlo_ewk = get_nlo_ewk_weight['a'](genIsoAs.pt.max())\n",
    "#                 for systematic in get_nnlo_nlo_weight['a']:\n",
    "#                     nnlo_nlo[systematic] = get_nnlo_nlo_weight['a'][systematic](genIsoAs.pt.max())*ak.values_astype((ak.num(genIsoAs,axis=1) > 0), np.int) + ak.values_astype(~(ak.num(genIsoAs, axis=1) > 0), np.int)\n",
    "            if ('WJetsToLNu' in dataset) & ('HT' in dataset):\n",
    "                nlo_qcd = get_nlo_qcd_weight['w'](ak.max(genWs.pt, axis=1))\n",
    "                nlo_ewk = get_nlo_ewk_weight['w'](ak.max(genWs.pt, axis=1))\n",
    "                for systematic in get_nnlo_nlo_weight['w']:\n",
    "                    nnlo_nlo[systematic] = get_nnlo_nlo_weight['w'][systematic](ak.max(genWs.pt))*ak.values_astype((ak.num(genWs,axis=1) > 0), np.int) + ak.values_astype(~(ak.num(genWs, axis=1) > 0), np.int)\n",
    "\n",
    "            elif('DY' in dataset):\n",
    "                nlo_qcd = get_nlo_qcd_weight['dy'](ak.max(genDYs.pt, axis=1))\n",
    "                nlo_ewk = get_nlo_ewk_weight['dy'](ak.max(genDYs.pt, axis=1))\n",
    "                for systematic in get_nnlo_nlo_weight['dy']:\n",
    "                    nnlo_nlo[systematic] = get_nnlo_nlo_weight['dy'][systematic](ak.max(genDYs.pt))*ak.values_astype((ak.num(genZs, axis=1) > 0), np.int) + ak.values_astype(~(ak.num(genZs, axis=1) > 0), np.int)\n",
    "            elif('ZJets' in dataset):\n",
    "                nlo_qcd = get_nlo_qcd_weight['z'](ak.max(genZs.pt, axis=1))\n",
    "                nlo_ewk = get_nlo_ewk_weight['z'](ak.max(genZs.pt, axis=1))\n",
    "                for systematic in get_nnlo_nlo_weight['z']:\n",
    "                    nnlo_nlo[systematic] = get_nnlo_nlo_weight['z'][systematic](ak.max(genZs.pt))*ak.values_astype((ak.num(genZs, axis=1) > 0), np.int) + ak.values_astype(~(ak.num(genZs, axis=1) > 0), np.int)\n",
    "\n",
    "            ###\n",
    "            # Calculate PU weight and systematic variations\n",
    "            ###\n",
    "\n",
    "            pu = get_pu_weight(events.Pileup.nTrueInt)\n",
    "\n",
    "            ###\n",
    "            # Trigger efficiency weight\n",
    "            ###\n",
    "\n",
    "#             e1sf = get_ele_trig_weight(leading_ele_pair.i0.eta.sum()+leading_ele_pair.i0.deltaEtaSC.sum(), leading_ele_pair.i0.pt.sum())*(leading_ele_pair.i0.pt.sum() > 40).astype(np.int)\n",
    "#             e2sf = get_ele_trig_weight(leading_ele_pair.i1.eta.sum()+leading_ele_pair.i1.deltaEtaSC.sum(), leading_ele_pair.i1.pt.sum())*(leading_ele_pair.i1.pt.sum() > 40).astype(np.int)\n",
    "\n",
    "#             if self._year == '2016':\n",
    "#                 sf = get_pho_trig_weight(leading_pho.pt.sum())\n",
    "#             elif self._year == '2017':  # Sigmoid used for 2017 and 2018, values from monojet\n",
    "#                 sf = sigmoid(leading_pho.pt.sum(), 0.335, 217.91, 0.065, 0.996) / sigmoid(leading_pho.pt.sum(), 0.244, 212.34, 0.050, 1.000)\n",
    "#                 sf[np.isnan(sf) | np.isinf(sf)] == 1\n",
    "#             elif self._year == '2018':\n",
    "#                 sf = sigmoid(leading_pho.pt.sum(), 1.022, 218.39, 0.086, 0.999) / sigmoid(leading_pho.pt.sum(), 0.301, 212.83, 0.062, 1.000)\n",
    "#                 sf[np.isnan(sf) | np.isinf(sf)] == 1\n",
    "\n",
    "#            trig = {\n",
    "#                'sre': get_ele_trig_weight(ak.sum(leading_e.eta+leading_e.deltaEtaSC, axis=-1), ak.sum(leading_e.pt,axis=-1)),\n",
    "#                'srm':get_mu_trig_weight(abs(ak.sum(leading_mu.eta, axis= -1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#                'ttbare': get_ele_trig_weight(ak.sum(leading_e.eta+leading_e.deltaEtaSC, axis=-1), ak.sum(leading_e.pt,axis=-1)),\n",
    "#                'ttbarm': get_mu_trig_weight(abs(ak.sum(leading_mu.eta, axis= -1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#                'wjete': get_ele_trig_weight(ak.sum(leading_e.eta+leading_e.deltaEtaSC, axis=-1), ak.sum(leading_e.pt,axis=-1)),\n",
    "#                'wjetm':get_mu_trig_weight(abs(ak.sum(leading_mu.eta, axis= -1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#            }\n",
    "#            trig_err = {\n",
    "#                'sre': get_ele_trig_err(ak.sum(leading_e.eta+leading_e.deltaEtaSC, axis=-1), ak.sum(leading_e.pt,axis=-1)),\n",
    "#                'srm':get_mu_trig_err(abs(ak.sum(leading_mu.eta, axis= -1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#                'ttbare': get_ele_trig_err(ak.sum(leading_e.eta+leading_e.deltaEtaSC, axis=-1), ak.sum(leading_e.pt,axis=-1)),\n",
    "#                'ttbarm': get_mu_trig_err(abs(ak.sum(leading_mu.eta, axis= -1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#                'wjete': get_ele_trig_err(ak.sum(leading_e.eta+leading_e.deltaEtaSC, axis=-1), ak.sum(leading_e.pt,axis=-1)),\n",
    "#                'wjetm':get_mu_trig_err(abs(ak.sum(leading_mu.eta, axis= -1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#            }\n",
    "            trig = {\n",
    "                'sr': get_met_trig_weight(met.pt),\n",
    "                'wmcr': np.ones(event_size),\n",
    "                'tmcr': np.ones(event_size),\n",
    "                'wecr': get_ele_trig_weight(ak.sum(leading_e.eta+leading_e.deltaEtaSC, axis=-1), ak.sum(leading_e.pt,axis=-1)),\n",
    "                'tecr': get_ele_trig_weight(ak.sum(leading_e.eta+leading_e.deltaEtaSC, axis=-1), ak.sum(leading_e.pt,axis=-1)),\n",
    "                'zmcr': np.ones(event_size),\n",
    "                'zecr': np.ones(event_size),\n",
    "                'gcr': np.ones(event_size)\n",
    "            }\n",
    "            trig_err = {\n",
    "                'sr': np.ones(event_size),\n",
    "                'wmcr': np.ones(event_size),\n",
    "                'tmcr': np.ones(event_size),\n",
    "                'wecr': np.ones(event_size),\n",
    "                'tecr': np.ones(event_size),\n",
    "                'zmcr': np.ones(event_size),\n",
    "                'zecr': np.ones(event_size),\n",
    "                'gcr': np.ones(event_size)\n",
    "            }\n",
    "\n",
    "            ###\n",
    "            # Calculating electron and muon ID weights\n",
    "            ###\n",
    "\n",
    "#             mueta = abs(leading_mu.eta.sum())\n",
    "#             mu1eta = abs(leading_mu_pair.i0.eta.sum())\n",
    "#             mu2eta = abs(leading_mu_pair.i1.eta.sum())\n",
    "#             if self._year == '2016':\n",
    "#                 mueta = leading_mu.eta.sum()\n",
    "#                 mu1eta = leading_mu_pair.i0.eta.sum()\n",
    "#                 mu2eta = leading_mu_pair.i1.eta.sum()\n",
    "            if self._year == '2016':\n",
    "                sf = get_pho_tight_id_sf(leading_pho.eta, leading_pho.pt)\n",
    "            else:  # 2017/2018 monojet measurement depends only on abs(eta)\n",
    "                sf = get_pho_tight_id_sf(abs(leading_pho.eta))\n",
    "\n",
    "#            ids = {\n",
    "#                'sre': get_ele_tight_id_sf(ak.sum(leading_e.eta, axis=-1), ak.sum(leading_e.pt, axis=-1)),\n",
    "#                'srm': get_mu_tight_id_sf(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#                'ttbare': get_ele_tight_id_sf(ak.sum(leading_e.eta, axis=-1), ak.sum(leading_e.pt, axis=-1)),\n",
    "#                'ttbarm': get_mu_tight_id_sf(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#                'wjete': get_ele_tight_id_sf(ak.sum(leading_e.eta, axis=-1), ak.sum(leading_e.pt, axis=-1)),\n",
    "#                'wjetm': get_mu_tight_id_sf(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#            }\n",
    "#            ids_err = {\n",
    "#                'sre': get_ele_tight_id_err(ak.sum(leading_e.eta, axis=-1), ak.sum(leading_e.pt, axis=-1)),\n",
    "#                'srm': get_mu_tight_id_err(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#                'ttbare': get_ele_tight_id_err(ak.sum(leading_e.eta, axis=-1), ak.sum(leading_e.pt, axis=-1)),\n",
    "#                'ttbarm': get_mu_tight_id_err(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#                'wjete': get_ele_tight_id_err(ak.sum(leading_e.eta, axis=-1), ak.sum(leading_e.pt, axis=-1)),\n",
    "#                'wjetm': get_mu_tight_id_err(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#            }\n",
    "\n",
    "            ids = {\n",
    "                'sr': np.ones(event_size),\n",
    "                'wmcr': get_mu_tight_id_sf(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "                'tmcr': get_mu_tight_id_sf(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "                'wecr': get_ele_tight_id_sf(ak.sum(leading_e.eta, axis=-1), ak.sum(leading_e.pt, axis=-1)),\n",
    "                'tecr': get_ele_tight_id_sf(ak.sum(leading_e.eta, axis=-1), ak.sum(leading_e.pt, axis=-1)),\n",
    "                'zmcr': get_mu_tight_id_sf(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),  ##!\n",
    "                'zecr': get_ele_tight_id_sf(ak.sum(leading_e.eta, axis=-1), ak.sum(leading_e.pt, axis=-1)),  ##!\n",
    "                'gcr': sf\n",
    "            }\n",
    "            ids_err = {\n",
    "                'sr': np.ones(event_size),\n",
    "                'wmcr': get_mu_tight_id_err(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "                'tmcr': get_mu_tight_id_err(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "                'wecr': get_ele_tight_id_err(ak.sum(leading_e.eta, axis=-1), ak.sum(leading_e.pt, axis=-1)),\n",
    "                'tecr': get_ele_tight_id_err(ak.sum(leading_e.eta, axis=-1), ak.sum(leading_e.pt, axis=-1)),\n",
    "                'zmcr': get_mu_tight_id_err(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)), ##!\n",
    "                'zecr': get_ele_tight_id_err(ak.sum(leading_e.eta, axis=-1), ak.sum(leading_e.pt, axis=-1)),  ##!\n",
    "                'gcr': np.ones(event_size)\n",
    "            }\n",
    "            ###\n",
    "            # Reconstruction weights for electrons\n",
    "            ###\n",
    "\n",
    "            # 2017 has separate weights for low/high pT (threshold at 20 GeV)\n",
    "            def ele_reco_sf(pt, eta): \n",
    "                return get_ele_reco_sf(eta, pt)*ak.values_astype((pt > 20), np.int) + get_ele_reco_lowet_sf(eta, pt)*ak.values_astype((~(pt > 20)), np.int)\n",
    "            \n",
    "            def ele_reco_err(pt, eta):\n",
    "                return get_ele_reco_err(eta, pt)*ak.values_astype((pt > 20), np.int) + get_ele_reco_lowet_err(eta, pt)*ak.values_astype((~(pt > 20)), np.int)\n",
    "\n",
    "            #look at this rRISHABH\n",
    "            if self._year == '2017' or self._year == '2018' or self._year == '2016':\n",
    "                sf = ele_reco_sf\n",
    "            else:\n",
    "                sf = get_ele_reco_sf\n",
    "\n",
    "#            reco = {\n",
    "#                'sre': sf(ak.sum(leading_e.eta+leading_e.deltaEtaSC, axis=-1), ak.sum(leading_e.pt, axis=-1)),\n",
    "#                'srm': np.ones(event_size),\n",
    "#                'ttbare': sf(ak.sum(leading_e.eta+leading_e.deltaEtaSC, axis=-1), ak.sum(leading_e.pt, axis=-1)),\n",
    "#                'ttbarm': np.ones(event_size),\n",
    "#                'wjete': sf(ak.sum(leading_e.eta+leading_e.deltaEtaSC, axis=-1), ak.sum(leading_e.pt, axis=-1)),\n",
    "#                'wjetm': np.ones(event_size),\n",
    "#            }\n",
    "#            reco_err = {\n",
    "#                'sre': ele_reco_err(ak.sum(leading_e.eta+leading_e.deltaEtaSC, axis=-1), ak.sum(leading_e.pt, axis=-1)),\n",
    "#                'srm': np.ones(event_size),\n",
    "#                'ttbare': ele_reco_err(ak.sum(leading_e.eta+leading_e.deltaEtaSC, axis=-1), ak.sum(leading_e.pt, axis=-1)),\n",
    "#                'ttbarm': np.ones(event_size),\n",
    "#                'wjete': ele_reco_err(ak.sum(leading_e.eta+leading_e.deltaEtaSC, axis=-1), ak.sum(leading_e.pt, axis=-1)),\n",
    "#                'wjetm': np.ones(event_size),\n",
    "#            }\n",
    "            reco = {\n",
    "                'sr': np.ones(event_size),\n",
    "                'wmcr': np.ones(event_size),\n",
    "                'tmcr': np.ones(event_size),\n",
    "                'wecr': np.ones(event_size),\n",
    "                'tecr': np.ones(event_size),\n",
    "                'zmcr': np.ones(event_size),\n",
    "                'zecr': np.ones(event_size),\n",
    "                'gcr': np.ones(event_size)\n",
    "            }\n",
    "            reco_err = {\n",
    "                'sr': np.ones(event_size),\n",
    "                'wmcr': np.ones(event_size),\n",
    "                'tmcr': np.ones(event_size),\n",
    "                'wecr': np.ones(event_size),\n",
    "                'tecr': np.ones(event_size),\n",
    "                'zmcr': np.ones(event_size),\n",
    "                'zecr': np.ones(event_size),\n",
    "                'gcr': np.ones(event_size)\n",
    "            }\n",
    "            ###\n",
    "            # Isolation weights for muons\n",
    "            ###\n",
    "\n",
    "#            isolation = {\n",
    "#                'sre': np.ones(event_size),\n",
    "#                'srm': get_mu_tight_iso_sf(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#                'ttbare': np.ones(event_size),\n",
    "#                'ttbarm': get_mu_tight_iso_sf(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#                'wjete': np.ones(event_size),\n",
    "#                'wjetm': get_mu_tight_iso_sf(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#            }\n",
    "#            isolation_err = {\n",
    "#                'sre': np.ones(event_size),\n",
    "#                'srm': get_mu_tight_iso_err(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#                'ttbare': np.ones(event_size),\n",
    "#                'ttbarm': get_mu_tight_iso_err(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#                'wjete': np.ones(event_size),\n",
    "#                'wjetm': get_mu_tight_iso_err(abs(ak.sum(leading_mu.eta, axis=-1)), ak.sum(leading_mu.pt, axis=-1)),\n",
    "#            }\n",
    "            isolation = {\n",
    "                'sr': np.ones(event_size),\n",
    "                'wmcr': np.ones(event_size),\n",
    "                'tmcr': np.ones(event_size),\n",
    "                'wecr': np.ones(event_size),\n",
    "                'tecr': np.ones(event_size),\n",
    "                'zmcr': np.ones(event_size),\n",
    "                'zecr': np.ones(event_size),\n",
    "                'gcr': np.ones(event_size)\n",
    "            }\n",
    "            isolation_err = {\n",
    "                'sr': np.ones(event_size),\n",
    "                'wmcr': np.ones(event_size),\n",
    "                'tmcr': np.ones(event_size),\n",
    "                'wecr': np.ones(event_size),\n",
    "                'tecr': np.ones(event_size),\n",
    "                'zmcr': np.ones(event_size),\n",
    "                'zecr': np.ones(event_size),\n",
    "                'gcr': np.ones(event_size)\n",
    "            }\n",
    "            ###\n",
    "            # CSEV weight for photons: https://twiki.cern.ch/twiki/bin/view/CMS/EgammaIDRecipesRun2#Electron_Veto_CSEV_or_pixel_seed\n",
    "            ###\n",
    "\n",
    "#             if self._year == '2016':\n",
    "#                 csev_weight = get_pho_csev_sf(abs(ak.sum(leading_pho.eta, axis=-1)), ak.sum(leading_pho.pt))\n",
    "#             elif self._year == '2017':\n",
    "#                 csev_sf_index = 0.5*(ak.values_astype(ak.sum(leading_pho.isScEtaEB, axis=-1), np.int))+3.5*(~(ak.values_astype(ak.sum(leading_pho.isScEtaEB, axis=-1), np.int)))+1*(ak.values_astype(ak.sum(leading_pho.r9, axis=-1) > 0.94), np.int)+2*(ak.values_astype(ak.sum(leading_pho.r9, axis=-1) <= 0.94), np.int)\n",
    "#                 csev_weight = get_pho_csev_sf(csev_sf_index)\n",
    "#             elif self._year == '2018':\n",
    "#                 csev_weight = get_pho_csev_sf(ak.sum(leading_pho.pt, axis=-1), abs(ak.sum(leading_pho.eta, axis=-1)))\n",
    "#             csev_weight[csev_weight == 0] = 1\n",
    "\n",
    "\n",
    "\n",
    "            ###\n",
    "            # AK4 b-tagging weights\n",
    "            ###\n",
    "            '''\n",
    "            if in a region you are asking for 0 btags, you have to apply the 0-btag weight\n",
    "            if in a region you are asking for at least 1 btag, you need to apply the -1-btag weight\n",
    "            it’s “-1” because we want to reserve “1\" to name the weight that should be applied when you ask for exactly 1 b-tag\n",
    "            that is different from the weight you apply when you ask for at least 1 b-tag\n",
    "            '''\n",
    "#             if 'preVFP' in dataset:\n",
    "#                 VFP_status = 'preVFP'\n",
    "#             elif 'postVFP' in dataset:\n",
    "#                 VFP_status = 'postVFP'\n",
    "#             else:\n",
    "# #                 VFP_status = False\n",
    "            btag = {}\n",
    "            btagUp = {}\n",
    "            btagDown = {}\n",
    "            btag['sre'],   btagUp['sre'],   btagDown['sre'] = get_deepflav_weight['medium'](j_good_clean,j_good_clean.pt, j_good_clean.eta, j_good_clean.hadronFlavour, '+1', )\n",
    "            btag['srm'],   btagUp['srm'],   btagDown['srm'] = get_deepflav_weight['medium'](j_good_clean,j_good_clean.pt, j_good_clean.eta, j_good_clean.hadronFlavour, '+1', )\n",
    "            btag['ttbare'], btagUp['ttbare'], btagDown['ttbare'] = get_deepflav_weight['medium'](j_good_clean, j_good_clean.pt, j_good_clean.eta, j_good_clean.hadronFlavour, '2', )\n",
    "            btag['ttbarm'], btagUp['ttbarm'], btagDown['ttbarm'] = get_deepflav_weight['medium'](j_good_clean, j_good_clean.pt, j_good_clean.eta, j_good_clean.hadronFlavour, '2', )\n",
    "            btag['wjete'], btagUp['wjete'], btagDown['wjete'] = get_deepflav_weight['medium'](j_good_clean, j_good_clean.pt, j_good_clean.eta, j_good_clean.hadronFlavour, '0', )\n",
    "            btag['wjetm'], btagUp['wjetm'], btagDown['wjetm'] = get_deepflav_weight['medium'](j_good_clean, j_good_clean.pt, j_good_clean.eta, j_good_clean.hadronFlavour, '0', )  \n",
    "\n",
    "\n",
    "        ###\n",
    "        # Selections\n",
    "        ###\n",
    "\n",
    "        met_filters = np.ones(event_size, dtype=np.bool)\n",
    "        # this filter is recommended for data only\n",
    "        if isData:\n",
    "            met_filters = met_filters & events.Flag['eeBadScFilter']\n",
    "        for flag in AnalysisProcessor.met_filter_flags[self._year]:\n",
    "            met_filters = met_filters & events.Flag[flag]\n",
    "        selection.add('met_filters', ak.to_numpy(met_filters, np.bool))\n",
    "\n",
    "        triggers = np.zeros(event_size, dtype=np.bool)\n",
    "        for path in self._met_triggers[self._year]:\n",
    "            if path not in events.HLT.fields:\n",
    "                continue\n",
    "            triggers = triggers | events.HLT[path]\n",
    "        selection.add('met_triggers', ak.to_numpy(triggers))\n",
    "\n",
    "        triggers = np.zeros(event_size, dtype=np.bool)\n",
    "        for path in self._singleelectron_triggers[self._year]:\n",
    "            if path not in events.HLT.fields:\n",
    "                continue\n",
    "            triggers = triggers | events.HLT[path]\n",
    "        selection.add('single_electron_triggers', ak.to_numpy(triggers))\n",
    "\n",
    "        triggers = np.zeros(event_size, dtype=np.bool)\n",
    "        for path in self._singlephoton_triggers[self._year]:\n",
    "            if path not in events.HLT.fields:\n",
    "                continue\n",
    "            triggers = triggers | events.HLT[path]\n",
    "        selection.add('single_photon_triggers', ak.to_numpy(triggers))\n",
    "\n",
    "        triggers = np.zeros(event_size, dtype=np.bool)\n",
    "        for path in self._singlemuon_triggers[self._year]:\n",
    "            if path not in events.HLT.fields:\n",
    "                continue\n",
    "            triggers = triggers | events.HLT[path]\n",
    "        selection.add('single_muon_triggers', ak.to_numpy(triggers))\n",
    "\n",
    "        noHEMj = np.ones(event_size, dtype=np.bool)\n",
    "        if self._year == '2018':\n",
    "            noHEMj = (j_nHEM == 0)\n",
    "\n",
    "        noHEMmet = np.ones(event_size, dtype=np.bool)\n",
    "        if self._year == '2018':\n",
    "            noHEMmet = (met.pt > 470) | (met.phi > -0.62) | (met.phi < -1.62)\n",
    "\n",
    "        '''\n",
    "        what the next 6 lines of code do:\n",
    "        main object is to exclude events from JetHt sample with W_pT b/w 70-100 GeV\n",
    "        events.metadata['dataset'] = 'WJetsToLNu_HT-100To200_TuneCP5_13TeV-madgraphMLM-pythia8____27_'\n",
    "        dataset = 'WJetsToLNu'\n",
    "        see if the 'HT' is in the name of the sample\n",
    "        so, it first goes to genpart,\n",
    "        figures out if the genlevel process is hardprocess and firstcopy and there are genlevel particle with\n",
    "        abs(pdgID)= 24\n",
    "        ad selects only those events for the pT of W was > 100 GeV\n",
    "        '''\n",
    "\n",
    "        # predeclration just in cas I don't want the filter\n",
    "        # selection.add(\"exclude_low_WpT_JetHT\", np.full(len(events), True))\n",
    "#         if ('WJetsToLNu' in dataset) & (events.metadata['dataset'].split('-')[0].split('_')[1] == 'HT'):\n",
    "\n",
    "#             GenPart = events.GenPart\n",
    "#             remove_overlap = (GenPart[GenPart.hasFlags(['fromHardProcess', 'isFirstCopy', 'isPrompt']) &\n",
    "#                                       ((abs(GenPart.pdgId) == 24))].pt > 100).all()\n",
    "#             selection.add(\"exclude_low_WpT_JetHT\", remove_overlap)\n",
    "\n",
    "#         else:\n",
    "#             selection.add(\"exclude_low_WpT_JetHT\", np.full(event_size, True))\n",
    "        if ('WJetsToLNu' in dataset) & ('Pt' in dataset):\n",
    "            remove_overlap = (gen[gen.hasFlags(['fromHardProcess', 'isFirstCopy']) & ((abs(gen.pdgId) == 24))].pt >= 200)\n",
    "            selection.add(\"exclude_wjets_greater_200\", ak.to_numpy(ak.sum(remove_overlap, axis=1)>0))\n",
    "        else:\n",
    "            selection.add(\"exclude_wjets_greater_200\", np.full(event_size, True))\n",
    "            \n",
    "        if ('WJetsToLNu' in dataset) & (not ('Pt' in dataset)):\n",
    "            remove_overlap = (gen[gen.hasFlags(['fromHardProcess', 'isFirstCopy']) & ((abs(gen.pdgId) == 24))].pt < 200)\n",
    "            selection.add(\"exclude_wjets_less_200\", ak.to_numpy(ak.sum(remove_overlap, axis=1)>0))\n",
    "        else:\n",
    "            selection.add(\"exclude_wjets_less_200\", np.full(event_size, True))\n",
    "\n",
    "        if ('DYJetsToLL' in dataset) & (not ('Pt' in dataset)):\n",
    "            remove_overlap = (gen[gen.hasFlags(['fromHardProcess', 'isFirstCopy']) & ((abs(gen.pdgId) == 23))].pt < 400)\n",
    "            selection.add(\"exclude_dyjets_less_400\", ak.to_numpy(ak.sum(remove_overlap, axis=1)>0))\n",
    "        else:\n",
    "            selection.add(\"exclude_dyjets_less_400\", np.full(event_size, True))\n",
    "\n",
    "        if ('DYJetsToLL' in dataset) & ('Pt' in dataset):\n",
    "            remove_overlap = (gen[gen.hasFlags(['fromHardProcess', 'isFirstCopy']) & ((abs(gen.pdgId) == 23))].pt >= 400)\n",
    "            selection.add(\"exclude_dyjets_greater_400\", ak.to_numpy(ak.sum(remove_overlap, axis=1)>0))\n",
    "        else:\n",
    "            selection.add(\"exclude_dyjets_greater_400\", np.full(event_size, True))\n",
    "\n",
    "        selection.add('DeltaR_LJ_mask',ak.to_numpy(DeltaR_LJ_Ele_mask | DeltaR_LJ_Mu_mask))\n",
    "        selection.add('isoneM', ak.to_numpy((e_nloose == 0) & (mu_ntight == 1) & ( mu_nloose == 1)))\n",
    "        selection.add('isoneE', ak.to_numpy((e_ntight == 1) & (e_nloose == 1) & (mu_nloose == 0)))\n",
    "        selection.add('iszeroL', ak.to_numpy((tau_nloose == 0) & (pho_nloose == 0) & (e_nloose == 0) & (mu_nloose == 0)))\n",
    "\n",
    "        selection.add('exactly_1_medium_btag', ak.to_numpy(j_ndflvM == 1))\n",
    "        selection.add('atleast_2_medium_btag', ak.to_numpy(j_ndflvM >= 2))\n",
    "        selection.add('zero_medium_btags', ak.to_numpy(j_ndflvM == 0))\n",
    "\n",
    "        selection.add('noHEMj', ak.to_numpy(noHEMj))\n",
    "        selection.add('noHEMmet', ak.to_numpy(noHEMmet))\n",
    "        selection.add('met80', ak.to_numpy(met.pt < 80))\n",
    "        selection.add('met100', ak.to_numpy(met.pt > 100))\n",
    "        selection.add('Delta_Phi_Met_LJ', ak.to_numpy(ak.sum(abs(met_LJ_Pairs.met.delta_phi(met_LJ_Pairs.lj))>1.5, axis=1)>0))\n",
    "        selection.add('DeltaR_LJ_Ele_mask', ak.to_numpy((DeltaR_LJ_Ele_mask)>0))\n",
    "\n",
    "        selection.add('one_muon', ak.to_numpy(ak.num(mu_tight, axis=1) == 1))\n",
    "        selection.add('zero_loose_electron', ak.to_numpy(ak.num(e_loose, axis=1) == 0))\n",
    "        selection.add('DeltaR_LJ_Mu_mask', ak.to_numpy((DeltaR_LJ_Mu_mask)>0))\n",
    "\n",
    "        for region in mT.keys():\n",
    "            sel_name = 'mt'+'_'+region+'>50'\n",
    "            select = (mT[region] > 50)\n",
    "            selection.add(sel_name, ak.to_numpy(ak.sum(select,axis=1)>0))\n",
    "        selection.add('leading_j>70',ak.to_numpy(ak.sum(leading_j.pt, axis=1) >70))# from the monotop paper\n",
    "        selection.add('atleast_one_jet_with_pt_grt_50',ak.to_numpy(atleast_one_jet_with_pt_grt_50))\n",
    "\n",
    "#         print(selection.all())\n",
    "#        regions = {\n",
    "#            'sre': ['isoneE', 'exactly_1_medium_btag', 'met_filters', 'single_electron_triggers', 'atleast_one_jet_with_pt_grt_50',\n",
    "#                     'Delta_Phi_Met_LJ', 'mt_sre>50','met100','noHEMj','noHEMmet',\"exclude_dyjets_less_400\", \"exclude_dyjets_greater_400\", \n",
    "#                    \"exclude_wjets_less_200\", \"exclude_wjets_greater_200\",\"leading_j>70\"],\n",
    "#            \n",
    "#            'srm': ['isoneM', 'exactly_1_medium_btag', 'met_filters', 'single_muon_triggers', 'atleast_one_jet_with_pt_grt_50',\n",
    "#                    'Delta_Phi_Met_LJ', 'mt_srm>50',  'met100', 'noHEMj','noHEMmet',\"exclude_dyjets_less_400\", \"exclude_dyjets_greater_400\", \n",
    "#                    \"exclude_wjets_less_200\", \"exclude_wjets_greater_200\",\"leading_j>70\"],\n",
    "#            \n",
    "#            'ttbare': ['isoneE', 'atleast_2_medium_btag', 'met_filters', 'single_electron_triggers', 'atleast_one_jet_with_pt_grt_50',\n",
    "#                       'Delta_Phi_Met_LJ', 'mt_ttbare>50', 'met100', 'noHEMj','noHEMmet' ,\"exclude_dyjets_less_400\", \"exclude_dyjets_greater_400\", \n",
    "#                       \"exclude_wjets_less_200\", \"exclude_wjets_greater_200\",\"leading_j>70\"],\n",
    "#            \n",
    "#            'ttbarm': ['isoneM', 'atleast_2_medium_btag', 'met_filters', 'single_muon_triggers', 'atleast_one_jet_with_pt_grt_50',\n",
    "#                        'Delta_Phi_Met_LJ', 'mt_ttbarm>50' , 'met100','noHEMj','noHEMmet',\"exclude_dyjets_less_400\", \"exclude_dyjets_greater_400\",\n",
    "#                       \"exclude_wjets_less_200\", \"exclude_wjets_greater_200\",\"leading_j>70\"],\n",
    "#            \n",
    "#            'wjete': [  'atleast_one_jet_with_pt_grt_50','met100','Delta_Phi_Met_LJ','zero_medium_btags','isoneE', 'met_filters', 'single_electron_triggers',\n",
    "#                       'mt_wjete>50' ,'noHEMj', 'noHEMmet', \"exclude_dyjets_less_400\", \"exclude_dyjets_greater_400\",\n",
    "#                      \"exclude_wjets_less_200\", \"exclude_wjets_greater_200\", \"leading_j>70\"],\n",
    "#            \n",
    "#            'wjetm': ['isoneM', 'zero_medium_btags', 'met_filters', 'single_muon_triggers', 'atleast_one_jet_with_pt_grt_50',\n",
    "#                      'Delta_Phi_Met_LJ', 'mt_wjetm>50', 'met100', 'noHEMj' ,'noHEMmet',\"exclude_dyjets_less_400\", \"exclude_dyjets_greater_400\",\n",
    "#                      \"exclude_wjets_less_200\", \"exclude_wjets_greater_200\", \"leading_j>70\"]\n",
    "#        }\n",
    "        regions = {\n",
    "            'sr': ['isoneE'],\n",
    "            'wmcr': ['isoneM'],\n",
    "            'tmcr': ['isoneM'],\n",
    "            'wecr': ['isoneE'],\n",
    "            'tecr': ['isoneE'],\n",
    "            'zmcr': ['isoneM'],\n",
    "            'zecr': ['isoneE'],\n",
    "            'gcr': ['isoneE']\n",
    "        }\n",
    "#     small code piece for checcking cutflow for after each selection\n",
    "#         mult = np.ones(len(events))\n",
    "#         for sel in regions['wjete']:\n",
    "#             print(sel,selection.all(sel),\"\\nSUM:\",sum(selection.all(sel)))\n",
    "#             mult *= selection.all(sel)\n",
    "#             print(\"events left\",sum(mult))\n",
    "#         print(\"MULT\", mult)\n",
    "#         print(\"SUM_MULT\", sum(mult))\n",
    "#         import sys\n",
    "#         sys.exit(0)\n",
    "        isFilled = False\n",
    "#         print(\"mu_ntight->\", mu_ntight.sum(),\n",
    "#               '\\n', 'e_ntight->', e_ntight.sum())\n",
    "        for region, cuts in regions.items():\n",
    "            if region not in selected_regions: continue\n",
    "            print('Considering region:', region)\n",
    "\n",
    "            ###\n",
    "            # Adding recoil and minDPhi requirements\n",
    "            ###\n",
    "\n",
    "            # selection.add('recoil_'+region, (u[region].mag>250))\n",
    "            # selection.add('mindphi_'+region, (abs(u[region].delta_phi(j_clean.T)).min()>0.8))\n",
    "            # regions[region].update({'recoil_'+region,'mindphi_'+region})\n",
    "            #             print('Selection:',regions[region])\n",
    "            variables = {\n",
    "\n",
    "                'mu_pT':              mu_tight.pt,\n",
    "                #'recoil':                 u[region],\n",
    "                # 'mindphirecoil':          abs(u[region].delta_phi(j_clean.T)).min(),\n",
    "                # 'CaloMinusPfOverRecoil':  abs(calomet.pt - met.pt) / u[region].mag,\n",
    "                'eT_miss':              met.pt,\n",
    "                'ele_pT':              e_tight.pt,\n",
    "#                 'jet_pT':              leading_j.pt,\n",
    "                'metphi':                 met.phi,\n",
    "                'dphi_Met_LJ':             Delta_Phi_Met_LJ_var,\n",
    "                'j1pt':                   leading_j.pt,\n",
    "                'j1eta':                  leading_j.eta,\n",
    "                'j1phi':                  leading_j.phi,\n",
    "                'fj1pt':                   leading_fj.pt,\n",
    "                'fj1eta':                  leading_fj.eta,\n",
    "                'fj1phi':                  leading_fj.phi,\n",
    "                # 'njets':                  j_nclean,\n",
    "                # 'ndflvL':                 j_ndflvL,\n",
    "                # 'ndcsvL':     j_ndcsvL,\n",
    "                # 'e1pt'      : leading_e.pt,\n",
    "                'ele_phi'     : leading_e.phi,\n",
    "                'ele_eta'     : leading_e.eta,\n",
    "                # 'dielemass' : leading_diele.mass,\n",
    "                # 'dielept'   : leading_diele.pt,\n",
    "                # 'mu1pt' : leading_mu.pt,\n",
    "                'mu_phi' : leading_mu.phi,\n",
    "                'mu_eta' : leading_mu.eta,\n",
    "                # 'dimumass' : leading_dimu.mass,\n",
    "                'dphi_e_etmiss':          abs(met.delta_phi(leading_e)),\n",
    "                'dphi_mu_etmiss':         abs(met.delta_phi(leading_mu)),\n",
    "                'dr_e_lj': DeltaR_LJ_Ele,\n",
    "                'dr_mu_lj': DeltaR_LJ_Mu,\n",
    "                'njets':                  j_ngood_clean,\n",
    "                'nfatjets':                  fj_nclean,\n",
    "                'ndflvM':                 j_ndflvM,\n",
    "                'TvsQCD':                 leading_fj.TvsQCD,\n",
    "#                 'ndcsvM':     j_ndcsvM,\n",
    "#                 'scale_factors': np.ones(event_size, dtype=np.bool)\n",
    "                }\n",
    "            if region in mT:\n",
    "                variables['mT'] = mT[region]\n",
    "#                 print(mT[region])\n",
    "#                 if 'e' in region[-1]:\n",
    "#                     WRF = leading_e.T.sum()-met.T\n",
    "#                 else:\n",
    "#                     pass\n",
    "#                     WRF = leading_mu.T.sum()-met.T\n",
    "#                 variables['recoilphiWRF'] = abs(u[region].delta_phi(WRF))\n",
    "#             print('Variables:', variables.keys())\n",
    "\n",
    "            def fill(dataset, weight, cut):\n",
    "\n",
    "                flat_variables = {k: ak.flatten(v[cut], axis=None) for k, v in variables.items()}\n",
    "                flat_weight = {k: ak.flatten(~np.isnan(v[cut])*weight[cut], axis=None) for k, v in variables.items()}\n",
    "#                 for k, v in variables.items():\n",
    "#                     print(k)\n",
    "#                     print(v)\n",
    "#                     print(cut)\n",
    "#                     print(\"v[cut]:\",v[cut])\n",
    "#                     print(\"ak.flatten(v[cut], axis=None):\",ak.flatten(v[cut], axis=None))\n",
    "#                     print(\"ak.flatten(v[cut]):\",ak.flatten(v[cut]))\n",
    "#                     print(\"~np.isnan(v[cut]\",~np.isnan(v[cut]))   \n",
    "#                     print(\"weight[cut]:\",weight[cut])\n",
    "#                     print(\"~np.isnan(v[cut]\",~np.isnan(v[cut]))\n",
    "#                     print(\"ak.flatten(~np.isnan(v[cut])*weight[cut]))\", ak.flatten(~np.isnan(v[cut])*weight[cut], axis=None))\n",
    "# #                     print(\"len(v*cut)\",len(v*cut))\n",
    "# #                     print(\"len(weight[cut])\",len(weight[cut]))\n",
    "# #                     print(\"len(~np.isnan(v[cut])*weight[cut])\",len(~np.isnan(v[cut])*weight[cut]))\n",
    "# #                     print((~np.isnan(v[cut])*weight[cut]))\n",
    "#                     import sys\n",
    "#                     sys.exit(0)\n",
    "                for histname, h in hout.items():\n",
    "#                     print(\"L1410\")\n",
    "                    if not isinstance(h, hist.Hist):\n",
    "                        continue\n",
    "                    if histname not in variables:\n",
    "                        continue\n",
    "                    elif histname == 'sumw':\n",
    "                        continue\n",
    "                    elif histname == 'template':\n",
    "                        continue\n",
    "#                     elif histname == 'scale_factors':\n",
    "#                         flat_variable = {histname: flat_weight[histname]}\n",
    "#                         h.fill(dataset=dataset,\n",
    "#                                region=region,\n",
    "#                                **flat_variable)\n",
    "\n",
    "                    else:\n",
    "                        flat_variable = {histname: flat_variables[histname]}\n",
    "#                         print(\"L1427\",flat_variable)\n",
    "                        h.fill(dataset=dataset,\n",
    "                               region=region,\n",
    "                               **flat_variable,\n",
    "                               weight=flat_weight[histname])\n",
    "\n",
    "            if isData:\n",
    "                if not isFilled:\n",
    "                    hout['sumw'].fill(dataset=dataset, sumw=1, weight=1)\n",
    "                    isFilled = True\n",
    "                cut = selection.all(*regions[region])\n",
    "                hout['template'].fill(dataset=dataset,\n",
    "                                      region=region,\n",
    "                                      systematic='nominal',\n",
    "                                      mT = mT[region],\n",
    "                                      weight=np.ones(event_size)*cut)\n",
    "                fill(dataset, np.ones(event_size), cut)\n",
    "            else:\n",
    "                weights = Weights(len(events))\n",
    "#                 print(\"L1445\")\n",
    "                if 'L1PreFiringWeight' in events.fields:\n",
    "                    weights.add('prefiring', events.L1PreFiringWeight.Nom)\n",
    "                weights.add('genw', events.genWeight)\n",
    "                weights.add('nlo_qcd', nlo_qcd)\n",
    "                weights.add('nlo_ewk', nlo_ewk)\n",
    "                weights.add('ttjet_weights', ttjet_weights)\n",
    "                if 'cen' in nnlo_nlo:\n",
    "                    #print('hi')\n",
    "                    weights.add('nnlo_nlo', nnlo_nlo['cen'])\n",
    "                    weights.add('qcd1', np.ones(\n",
    "                        event_size), nnlo_nlo['qcd1up']/nnlo_nlo['cen'], nnlo_nlo['qcd1do']/nnlo_nlo['cen'])\n",
    "                    weights.add('qcd2', np.ones(\n",
    "                        event_size), nnlo_nlo['qcd2up']/nnlo_nlo['cen'], nnlo_nlo['qcd2do']/nnlo_nlo['cen'])\n",
    "                    weights.add('qcd3', np.ones(\n",
    "                        event_size), nnlo_nlo['qcd3up']/nnlo_nlo['cen'], nnlo_nlo['qcd3do']/nnlo_nlo['cen'])\n",
    "                    weights.add('ew1', np.ones(\n",
    "                        event_size), nnlo_nlo['ew1up']/nnlo_nlo['cen'], nnlo_nlo['ew1do']/nnlo_nlo['cen'])\n",
    "                    weights.add('ew2G', np.ones(\n",
    "                        event_size), nnlo_nlo['ew2Gup']/nnlo_nlo['cen'], nnlo_nlo['ew2Gdo']/nnlo_nlo['cen'])\n",
    "                    weights.add('ew3G', np.ones(\n",
    "                        event_size), nnlo_nlo['ew3Gup']/nnlo_nlo['cen'], nnlo_nlo['ew3Gdo']/nnlo_nlo['cen'])\n",
    "                    weights.add('ew2W', np.ones(\n",
    "                        event_size), nnlo_nlo['ew2Wup']/nnlo_nlo['cen'], nnlo_nlo['ew2Wdo']/nnlo_nlo['cen'])\n",
    "                    weights.add('ew3W', np.ones(\n",
    "                        event_size), nnlo_nlo['ew3Wup']/nnlo_nlo['cen'], nnlo_nlo['ew3Wdo']/nnlo_nlo['cen'])\n",
    "                    weights.add('ew2Z', np.ones(\n",
    "                        event_size), nnlo_nlo['ew2Zup']/nnlo_nlo['cen'], nnlo_nlo['ew2Zdo']/nnlo_nlo['cen'])\n",
    "                    weights.add('ew3Z', np.ones(\n",
    "                        event_size), nnlo_nlo['ew3Zup']/nnlo_nlo['cen'], nnlo_nlo['ew3Zdo']/nnlo_nlo['cen'])\n",
    "                    weights.add('mix', np.ones(\n",
    "                        event_size), nnlo_nlo['mixup']/nnlo_nlo['cen'], nnlo_nlo['mixdo']/nnlo_nlo['cen'])\n",
    "                    weights.add('muF', np.ones(\n",
    "                        event_size), nnlo_nlo['muFup']/nnlo_nlo['cen'], nnlo_nlo['muFdo']/nnlo_nlo['cen'])\n",
    "                    weights.add('muR', np.ones(\n",
    "                        event_size), nnlo_nlo['muRup']/nnlo_nlo['cen'], nnlo_nlo['muRdo']/nnlo_nlo['cen'])\n",
    "                weights.add('pileup', pu)\n",
    "                \n",
    "                trig_name = str()\n",
    "                ids_name = str()\n",
    "                reco_name = str()\n",
    "                isolation_name = str()\n",
    "                if 'e' == region[-1]:\n",
    "                    trig_name = 'trig_e'\n",
    "                elif 'm' == region[-1]:\n",
    "                    trig_name = 'trig_m'\n",
    "                weights.add(trig_name, trig[region],trig[region]+trig_err[region], trig[region]-trig_err[region])\n",
    "                if 'e' == region[-1]:\n",
    "                    ids_name = 'id_e'\n",
    "                elif 'm' == region[-1]:\n",
    "                    ids_name = 'id_m'                \n",
    "                weights.add(ids_name, ids[region], ids[region]+ids_err[region], ids[region]-ids_err[region])\n",
    "                \n",
    "                if 'e' == region[-1]:\n",
    "                    reco_name = 'reco_e'\n",
    "                elif 'm' == region[-1]:\n",
    "                    reco_name = 'reco_m'\n",
    "                weights.add(reco_name, reco[region], reco[region]+reco_err[region], reco[region]-reco_err[region])\n",
    "                \n",
    "                if 'e' == region[-1]:\n",
    "                    isolation_name = 'isolation_e'\n",
    "                elif 'm' == region[-1]:\n",
    "                    isolation_name = 'isolation_m'                \n",
    "                weights.add(isolation_name, isolation[region], isolation[region]+isolation_err[region], isolation[region]-isolation_err[region])\n",
    "#                 weights.add('csev', csev[region])\n",
    "#                 weights.add('btag', btag[region],btagUp[region], btagDown[region])\n",
    "\n",
    "                if 'WJets' in dataset or 'DY' in dataset or 'ZJets' in dataset or 'GJets' in dataset:\n",
    "                    if not isFilled:\n",
    "#                         print(events.genWeight)\n",
    "#                         print(len(events.genWeight))\n",
    "                        hout['sumw'].fill(dataset='HF--'+dataset, sumw=1, weight=ak.sum(events.genWeight))\n",
    "                        hout['sumw'].fill(dataset='LF--'+dataset, sumw=1, weight=ak.sum(events.genWeight))\n",
    "                        isFilled = True\n",
    "                    whf = ak.values_astype(((ak.num(gen[gen.isb],axis=1) > 0) | (ak.num(gen[gen.isc], axis=1) > 0)), np.int)\n",
    "                    wlf = ak.values_astype(~(ak.values_astype(whf,np.bool)), np.int)\n",
    "                    cut = ak.to_numpy(selection.all(*regions[region]))\n",
    "#                     import sys\n",
    "#                     print(weights._modifiers.keys())\n",
    "#                     sys.exit(0)\n",
    "                    if 'WJets' in dataset:\n",
    "                        systematics = [None,\n",
    "#                                    'btagUp',\n",
    "#                                    'btagDown',\n",
    "                                       trig_name+'Up', trig_name+'Down',\n",
    "                                       ids_name+'Up', ids_name+'Down',\n",
    "                                       reco_name+'Up', reco_name+'Down',\n",
    "                                       isolation_name+'Up', isolation_name+'Down',\n",
    "                                      ]\n",
    "                    else:\n",
    "                        systematics = [None,\n",
    "#                                        'btagUp',\n",
    "#                                        'btagDown',\n",
    "                                       'qcd1Up',\n",
    "                                       'qcd1Down',\n",
    "                                       'qcd2Up',\n",
    "                                       'qcd2Down',\n",
    "                                       'qcd3Up',\n",
    "                                       'qcd3Down',\n",
    "                                       'muFUp',\n",
    "                                       'muFDown',\n",
    "                                       'muRUp',\n",
    "                                       'muRDown',\n",
    "                                       'ew1Up',\n",
    "                                       'ew1Down',\n",
    "                                       'ew2GUp',\n",
    "                                       'ew2GDown',\n",
    "                                       'ew2WUp',\n",
    "                                       'ew2WDown',\n",
    "                                       'ew2ZUp',\n",
    "                                       'ew2ZDown',\n",
    "                                       'ew3GUp',\n",
    "                                       'ew3GDown',\n",
    "                                       'ew3WUp',\n",
    "                                       'ew3WDown',\n",
    "                                       'ew3ZUp',\n",
    "                                       'ew3ZDown',\n",
    "                                       'mixUp',\n",
    "                                       'mixDown',\n",
    "                                       trig_name+'Up', trig_name+'Down',\n",
    "                                       ids_name+'Up', ids_name+'Down',\n",
    "                                       reco_name+'Up', reco_name+'Down',\n",
    "                                       isolation_name+'Up', isolation_name+'Down',\n",
    "                                      ]\n",
    "                    for systematic in systematics:\n",
    "                        sname = 'nominal' if systematic is None else systematic\n",
    "#                         import sys\n",
    "#                         print('weights.weight(modifier=systematic)', weights.weight(modifier=systematic))\n",
    "#                         print('whf', whf)\n",
    "#                         print(\"cut\", cut)\n",
    "#                         print(\"len ->weights.weight(modifier=systematic)\", len(weights.weight(modifier=systematic)))\n",
    "#                         print(\"len ->whf\", len(whf))\n",
    "#                         print(\"len ->cut\", len(cut))\n",
    "#                         x = weights.weight(modifier=systematic)*whf\n",
    "#                         print('weights.weight(modifier=systematic)*whf', weights.weight(modifier=systematic)*whf)\n",
    "#                         print('x[cut]', x[cut])\n",
    "#                         print(len(cut))\n",
    "#                         print(sum(ak.sum(mT[region], axis=-1)*x*cut))\n",
    "#                         sys.exit(0)\n",
    "                        hout['template'].fill(dataset='HF--'+dataset,\n",
    "                                              region=region,\n",
    "                                              systematic=sname,\n",
    "                                              mT = ak.sum(mT[region], axis=-1),\n",
    "                                              weight=weights.weight(modifier=systematic)*whf*cut)\n",
    "    \n",
    "                        hout['template'].fill(dataset='LF--'+dataset,\n",
    "                                              region=region,\n",
    "                                              systematic=sname,\n",
    "                                              mT = ak.sum(mT[region], axis=-1),\n",
    "                                              weight=weights.weight(modifier=systematic)*wlf*cut)\n",
    "                    ## Cutflow loop\n",
    "                    vcut=np.zeros(event_size, dtype=np.int)\n",
    "                    hout['cutflow'].fill(dataset='HF--'+dataset, region=region, cut=vcut, weight=weights.weight()*whf)\n",
    "                    hout['cutflow'].fill(dataset='LF--'+dataset, region=region, cut=vcut, weight=weights.weight()*wlf)\n",
    "                    allcuts = set()\n",
    "                    for i, icut in enumerate(cuts):\n",
    "                        allcuts.add(icut)\n",
    "                        jcut = selection.all(*allcuts)\n",
    "                        vcut = (i+1)*jcut\n",
    "                        hout['cutflow'].fill(dataset='HF--'+dataset, region=region, cut=vcut, weight=weights.weight()*jcut*whf)\n",
    "                        hout['cutflow'].fill(dataset='LF--'+dataset, region=region, cut=vcut, weight=weights.weight()*jcut*wlf)\n",
    "                    fill('HF--'+dataset, weights.weight()*whf, cut)\n",
    "                    fill('LF--'+dataset, weights.weight()*wlf, cut)\n",
    "\n",
    "                else:\n",
    "                    if not isFilled:\n",
    "                        hout['sumw'].fill(dataset=dataset, sumw=1, weight=ak.sum(events.genWeight, axis=-1))\n",
    "                        isFilled = True\n",
    "                    cut = selection.all(*regions[region])\n",
    "#                     for systematic in [None]:\n",
    "                    for systematic in [None,\n",
    "#                                    'btagUp',\n",
    "#                                    'btagDown',\n",
    "                                       trig_name+'Up', trig_name+'Down',\n",
    "                                       ids_name+'Up', ids_name+'Down',\n",
    "                                       reco_name+'Up', reco_name+'Down',\n",
    "                                       isolation_name+'Up', isolation_name+'Down',\n",
    "                                      ]:\n",
    "                        sname = 'nominal' if systematic is None else systematic\n",
    "                        hout['template'].fill(dataset=dataset,\n",
    "                                              region=region,\n",
    "                                              systematic=sname,\n",
    "                                              mT = ak.sum(mT[region], axis=-1),\n",
    "                                              weight=weights.weight(modifier=systematic)*cut)\n",
    "                        ## Cutflow loop\n",
    "                    vcut=np.zeros(event_size, dtype=np.int)\n",
    "                    hout['cutflow'].fill(dataset=dataset, region=region, cut=vcut, weight=weights.weight())\n",
    "                    allcuts = set()\n",
    "                    for i, icut in enumerate(cuts):\n",
    "                        allcuts.add(icut)\n",
    "                        jcut = selection.all(*allcuts)\n",
    "                        vcut = (i+1)*jcut\n",
    "                        hout['cutflow'].fill(dataset=dataset, region=region, cut=vcut, weight=weights.weight()*jcut)\n",
    "                    fill(dataset, weights.weight(), cut)\n",
    "#         time.sleep(0.5)\n",
    "        return hout\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        scale = {}\n",
    "        for d in accumulator['sumw'].identifiers('dataset'):\n",
    "            print('Scaling:', d.name)\n",
    "            dataset = d.name\n",
    "            if '--' in dataset:\n",
    "                dataset = dataset.split('--')[1]\n",
    "            print('Cross section:', self._xsec[dataset])\n",
    "            if self._xsec[dataset] != -1:\n",
    "#                 scale[d.name] = 1\n",
    "                scale[d.name] = self._lumi*self._xsec[dataset]\n",
    "            else:\n",
    "                scale[d.name] = 1\n",
    "\n",
    "        for histname, h in accumulator.items():\n",
    "            if histname == 'sumw':\n",
    "                continue\n",
    "            if isinstance(h, hist.Hist):\n",
    "                h.scale(scale, axis='dataset')\n",
    "\n",
    "        return accumulator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: DYJetsToLL_0J_TuneCP5_13TeV-amcatnloFXFX-pythia8____0_\n",
      "Processing: G1Jet_LHEGpT-150To250_TuneCP5_13TeV-amcatnlo-pythia8____0_\n",
      "Processing: Mphi-1995_Mchi-1000_2018____0_\n"
     ]
    }
   ],
   "source": [
    "fileslice = slice(None)\n",
    "with open('metadata/onefiles.json') as fin:\n",
    "    samplefiles = json.load(fin)\n",
    "    xsec = {k: v['xs'] for k, v in samplefiles.items()}\n",
    "\n",
    "for dataset, info in samplefiles.items():\n",
    "    filelist = {}\n",
    "    print('Processing:',dataset)\n",
    "    files = []\n",
    "    for file in info['files'][fileslice]:\n",
    "        files.append(file)\n",
    "    filelist[dataset] = files\n",
    "    \n",
    "corrections = load('data/corrections.coffea')\n",
    "ids = load('data/ids.coffea')\n",
    "common = load('data/common.coffea')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c224f0440d4b3dacb8f908c357e520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing:   0%|          | 0/1 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100afd466f0843ebad00279f48580f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/2 [00:00<?, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/awkward/_connect/_numpy.py:210: RuntimeWarning: invalid value encountered in true_divide\n",
      "  *[nplike.asarray(x) for x in inputs], **kwargs\n",
      "/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/awkward/_connect/_numpy.py:210: RuntimeWarning: invalid value encountered in true_divide\n",
      "  *[nplike.asarray(x) for x in inputs], **kwargs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qWT.pt, phi:  [[None, None, 18.8, 259], [None, None, ... None, None, None, None, None, None, None]] [[None, None, -0.0828, -1.09], [None, ... None, None, None, None, None, None, None]]\n",
      "gen.pdgId:  [[21, 2, 5000001, 6, 6, 5000001, 6, 22, ... 211, 211, 211, 4212, 22, 4122, 11, 11]]\n",
      "hasFlags:  [[GenParticle, GenParticle, GenParticle, ... GenParticle, GenParticle, GenParticle]]\n",
      "distinctparent:  [[None, None, 21, 21, 21, 21, 21, 6, ... -15, -15, -15, -511, 221, -4212, 2, 2]]\n",
      "qFromW:  [[None, None, GenParticle, GenParticle], ... None, None, None, None, None, None]]\n",
      "qWT.pt, phi:  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/awkward/_connect/_numpy.py:210: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  *[nplike.asarray(x) for x in inputs], **kwargs\n",
      "/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/awkward/_connect/_numpy.py:210: RuntimeWarning: invalid value encountered in multiply\n",
      "  *[nplike.asarray(x) for x in inputs], **kwargs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen[R_dyn]:  [[nan, nan, -999, -999, -999, -999, -999, ... -999, -999, -999, -999, -999, -999]]\n",
      "gen[R_0_dyn]:  [[nan, nan, -999, -999, -999, -999, -999, ... -999, -999, -999, -999, -999, -999]]\n",
      "[[None, None, None, 68, 43.6, None, None, ... 244, 358, None, None, None, None]] genhadrons.gen fields ['eta', 'mass', 'phi', 'pt', 'genPartIdxMother', 'pdgId', 'status', 'statusFlags', 'genPartIdxMotherG', 'distinctParentIdxG', 'childrenIdxG', 'distinctChildrenIdxG', 'isb', 'isc', 'isTop', 'isW', 'isZ', 'isA', 'R_dyn', 'R_0_dyn']\n",
      "genhadrons.hadrons fields ['eta', 'mass', 'phi', 'pt', 'genPartIdxMother', 'pdgId', 'status', 'statusFlags', 'genPartIdxMotherG', 'distinctParentIdxG', 'childrenIdxG', 'distinctChildrenIdxG', 'isb', 'isc', 'isTop', 'isW', 'isZ', 'isA', 'R_dyn', 'R_0_dyn']\n",
      "[[None, None, None, 2.95, -1.74, None, None, ... 2.67, 2.41, None, None, None, None]]\n",
      "gen.pdgId:  [[21, 2, 5000001, 6, 21, 5000001, 6, 5000521, ... 12, 11, 22, 421, 22, 22, 423, 421]]\n",
      "hasFlags:  [[GenParticle, GenParticle, GenParticle, ... GenParticle, GenParticle, GenParticle]]\n",
      "distinctparent:  [[None, None, 21, 21, None, 21, 21, ... -511, 111, -423, 111, 111, -511, 423]]\n",
      "qFromW:  [[None, None, None, GenParticle, GenParticle, ... None, None, None, None]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/awkward/_connect/_numpy.py:210: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  *[nplike.asarray(x) for x in inputs], **kwargs\n",
      "/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/awkward/_connect/_numpy.py:210: RuntimeWarning: invalid value encountered in multiply\n",
      "  *[nplike.asarray(x) for x in inputs], **kwargs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen[R_dyn]:  [[nan, nan, -999, -999, -999, -999, -999, ... -999, -999, -999, -999, -999, -999]]\n",
      "gen[R_0_dyn]:  [[nan, nan, -999, -999, -999, -999, -999, ... -999, -999, -999, -999, -999, -999]]\n",
      "genhadrons.gen fields ['eta', 'mass', 'phi', 'pt', 'genPartIdxMother', 'pdgId', 'status', 'statusFlags', 'genPartIdxMotherG', 'distinctParentIdxG', 'childrenIdxG', 'distinctChildrenIdxG', 'isb', 'isc', 'isTop', 'isW', 'isZ', 'isA', 'R_dyn', 'R_0_dyn']\n",
      "genhadrons.hadrons fields ['eta', 'mass', 'phi', 'pt', 'genPartIdxMother', 'pdgId', 'status', 'statusFlags', 'genPartIdxMotherG', 'distinctParentIdxG', 'childrenIdxG', 'distinctChildrenIdxG', 'isb', 'isc', 'isTop', 'isW', 'isZ', 'isA', 'R_dyn', 'R_0_dyn']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in ListOffsetArray64, cannot broadcast nested list\n\n(https://github.com/scikit-hep/awkward-1.0/blob/1.7.0/src/cpu-kernels/awkward_ListArray_broadcast_tooffsets.cpp#L27)\n\nFailed processing file: WorkItem(dataset='Mphi-1995_Mchi-1000_2018____0_', filename='/cms/scratch/sdogra/mtop/rishabh/decaf/analysis/Mphi-1995_Mchi-1000_2018_1.root', treename='Events', entrystart=0, entrystop=100000, fileuuid=b'\\x02\\x81\\xb3Rw\\xa7\\x11\\xec\\x82?,\\xbe\\xe1\\x83\\xbe\\xef', usermeta={})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/cvmfs/sft.cern.ch/lcg/releases/Python/3.6.5-f74f0/x86_64-centos7-gcc8-opt/lib/python3.6/concurrent/futures/process.py\", line 175, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/coffea/processor/executor.py\", line 197, in __call__\n    out = self.function(*args, **kwargs)\n  File \"/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/coffea/processor/executor.py\", line 990, in automatic_retries\n    raise e\n  File \"/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/coffea/processor/executor.py\", line 978, in automatic_retries\n    return func(*args, **kwargs)\n  File \"/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/coffea/processor/executor.py\", line 1240, in _work_function\n    ) from None\n  File \"/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/coffea/processor/executor.py\", line 1235, in _work_function\n    out = processor_instance.process(events)\n  File \"<ipython-input-2-16a6b104b8eb>\", line 1013, in process\n    isIsoA = isIsoA & isolation(gen.R_0_dyn*i/iterations)\n  File \"<ipython-input-2-16a6b104b8eb>\", line 1005, in isolation\n    mask_gen_had = abs(genhadrons.gen.delta_r(genhadrons.hadrons)) <=R\n  File \"/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/numpy/lib/mixins.py\", line 21, in func\n    return ufunc(self, other)\n  File \"/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/awkward/highlevel.py\", line 1418, in __array_ufunc__\n    return ak._connect._numpy.array_ufunc(ufunc, method, inputs, kwargs)\n  File \"/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/awkward/_connect/_numpy.py\", line 265, in array_ufunc\n    inputs, getfunction, behavior, allow_records=False, pass_depth=False\n  File \"/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/awkward/_util.py\", line 1182, in broadcast_and_apply\n    out = apply(broadcast_pack(inputs, isscalar), 0, user)\n  File \"/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/awkward/_util.py\", line 944, in apply\n    outcontent = apply(nextinputs, depth + 1, user)\n  File \"/cms/ldap_home/sdogra/.local/lib/python3.6/site-packages/awkward/_util.py\", line 985, in apply\n    nextinputs.append(x.broadcast_tooffsets64(offsets).content)\nValueError: in ListOffsetArray64, cannot broadcast nested list\n\n(https://github.com/scikit-hep/awkward-1.0/blob/1.7.0/src/cpu-kernels/awkward_ListArray_broadcast_tooffsets.cpp#L27)\n\nFailed processing file: WorkItem(dataset='Mphi-1995_Mchi-1000_2018____0_', filename='/cms/scratch/sdogra/mtop/rishabh/decaf/analysis/Mphi-1995_Mchi-1000_2018_1.root', treename='Events', entrystart=0, entrystop=100000, fileuuid=b'\\x02\\x81\\xb3Rw\\xa7\\x11\\xec\\x82?,\\xbe\\xe1\\x83\\xbe\\xef', usermeta={})\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-541aea85f3c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures_executor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#{\"schema\": NanoAODSchema},\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mexecutor_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'schema'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNanoAODSchema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'workers'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/coffea/processor/__init__.py\u001b[0m in \u001b[0;36m_run_x_job\u001b[0;34m(fileset, treename, processor_instance, executor, executor_args, pre_executor, pre_args, chunksize, maxchunks, metadata_cache, dynamic_chunksize, format)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mfileset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mtreename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mprocessor_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessor_instance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     )\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fileset, treename, processor_instance)\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mexe_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1366\u001b[0;31m         \u001b[0mwrapped_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[0mprocessor_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"out\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, items, function, accumulator)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0;31m# assume its a class then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpoolinstance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprocesswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpoolinstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mprocesswith\u001b[0;34m(pool)\u001b[0m\n\u001b[1;32m    567\u001b[0m                         \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m                     ),\n\u001b[0;32m--> 569\u001b[0;31m                     \u001b[0maccumulator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m                 )\n\u001b[1;32m    571\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/coffea/processor/accumulator.py\u001b[0m in \u001b[0;36maccumulate\u001b[0;34m(items, accum)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maccum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0maccum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;31m# we want to produce a new object so that the input is not mutated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0maccum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/coffea/processor/accumulator.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAccumulatable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAccumulatable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m ) -> Optional[Accumulatable]:\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maccum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36m_futures_handler\u001b[0;34m(futures, timeout)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCancelledError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/releases/Python/3.6.5-f74f0/x86_64-centos7-gcc8-opt/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/releases/Python/3.6.5-f74f0/x86_64-centos7-gcc8-opt/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in ListOffsetArray64, cannot broadcast nested list\n\n(https://github.com/scikit-hep/awkward-1.0/blob/1.7.0/src/cpu-kernels/awkward_ListArray_broadcast_tooffsets.cpp#L27)\n\nFailed processing file: WorkItem(dataset='Mphi-1995_Mchi-1000_2018____0_', filename='/cms/scratch/sdogra/mtop/rishabh/decaf/analysis/Mphi-1995_Mchi-1000_2018_1.root', treename='Events', entrystart=0, entrystop=100000, fileuuid=b'\\x02\\x81\\xb3Rw\\xa7\\x11\\xec\\x82?,\\xbe\\xe1\\x83\\xbe\\xef', usermeta={})"
     ]
    }
   ],
   "source": [
    "result = processor.run_uproot_job(\n",
    "    filelist,\n",
    "    \"Events\",\n",
    "    AnalysisProcessor('2018', xsec=xsec,\n",
    "                      corrections=corrections,\n",
    "                      ids=ids,\n",
    "                      common=common),\n",
    "    #processor.iterative_executor,\n",
    "    processor.futures_executor,\n",
    "    #{\"schema\": NanoAODSchema},\n",
    "    executor_args={'schema': NanoAODSchema,'workers': 8},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = OptionParser()\n",
    "    parser.add_option('-y', '--year', help='year', dest='year')\n",
    "    (options, args) = parser.parse_args()\n",
    "\n",
    "    #with open('metadata/UL_'+options.year+'.json') as fin:\n",
    "    with open('metadata/onefiles.json') as fin:\n",
    "        samplefiles = json.load(fin)\n",
    "        xsec = {k: v['xs'] for k, v in samplefiles.items()}\n",
    "\n",
    "    corrections = load('data/corrections.coffea')\n",
    "    ids = load('data/ids.coffea')\n",
    "    common = load('data/common.coffea')\n",
    "\n",
    "    processor_instance = AnalysisProcessor(year=options.year,\n",
    "                                           xsec=xsec,\n",
    "                                           corrections=corrections,\n",
    "                                           ids=ids,\n",
    "                                           common=common)\n",
    "\n",
    "    save(processor_instance, 'data/UL_had_test_'+options.year+'.processor')\n",
    "    print(\"processor name: UL_had_test_{}\".format(options.year))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
